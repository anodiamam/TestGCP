
==> Audit <==
|---------|--------------------------------|----------|-------------------|---------|---------------------|---------------------|
| Command |              Args              | Profile  |       User        | Version |     Start Time      |      End Time       |
|---------|--------------------------------|----------|-------------------|---------|---------------------|---------------------|
| start   | --driver=hyperv                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 14:09 IST |                     |
| start   | --driver=hyperv                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 14:10 IST |                     |
| start   |                                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 14:14 IST |                     |
| start   |                                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 15:10 IST |                     |
| start   |                                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 15:12 IST |                     |
| start   |                                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 15:19 IST |                     |
| start   | --vm-driver=hyperv --v=7       | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 15:20 IST |                     |
|         | --alsologtostderr              |          |                   |         |                     |                     |
| start   |                                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 15:21 IST |                     |
| delete  |                                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 15:24 IST | 16 May 24 15:24 IST |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 15:25 IST | 16 May 24 15:29 IST |
| start   |                                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 15:29 IST |                     |
| start   | --driver=hyperv                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 15:36 IST |                     |
| delete  |                                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 15:37 IST | 16 May 24 15:37 IST |
| start   | --driver=hyperv                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 15:37 IST |                     |
| start   | --driver=hyperv                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 15:39 IST |                     |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 15:40 IST |                     |
| delete  |                                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 15:40 IST | 16 May 24 15:40 IST |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 15:40 IST | 16 May 24 15:41 IST |
| ip      |                                | minikube | DEBASHISH-PC\User | v1.33.1 | 16 May 24 15:44 IST | 16 May 24 15:44 IST |
| start   |                                | minikube | DEBASHISH-PC\User | v1.33.1 | 17 May 24 13:03 IST | 17 May 24 13:03 IST |
| start   |                                | minikube | DEBASHISH-PC\User | v1.33.1 | 17 May 24 13:09 IST | 17 May 24 13:25 IST |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 17 May 24 15:35 IST |                     |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 17 May 24 15:52 IST |                     |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 17 May 24 15:52 IST | 17 May 24 15:55 IST |
| ip      |                                | minikube | DEBASHISH-PC\User | v1.33.1 | 17 May 24 15:57 IST | 17 May 24 15:57 IST |
| ip      |                                | minikube | DEBASHISH-PC\User | v1.33.1 | 17 May 24 15:58 IST | 17 May 24 15:58 IST |
| service | new-svc                        | minikube | DEBASHISH-PC\User | v1.33.1 | 17 May 24 18:11 IST | 17 May 24 18:21 IST |
| service | new-svc                        | minikube | DEBASHISH-PC\User | v1.33.1 | 17 May 24 18:21 IST |                     |
| service | new-svc                        | minikube | DEBASHISH-PC\User | v1.33.1 | 17 May 24 18:37 IST |                     |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 19 May 24 20:48 IST |                     |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 19 May 24 20:59 IST |                     |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 19 May 24 21:03 IST |                     |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 19 May 24 21:03 IST | 19 May 24 21:05 IST |
| service | new-svc                        | minikube | DEBASHISH-PC\User | v1.33.1 | 19 May 24 21:09 IST |                     |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 19 May 24 21:28 IST |                     |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 19 May 24 22:11 IST |                     |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 19 May 24 22:31 IST |                     |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 21 May 24 12:55 IST |                     |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 21 May 24 12:56 IST |                     |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 21 May 24 13:09 IST |                     |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 21 May 24 13:28 IST |                     |
| start   | --driver=docker                | minikube | DEBASHISH-PC\User | v1.33.1 | 21 May 24 13:32 IST |                     |
|---------|--------------------------------|----------|-------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/05/21 13:32:29
Running on machine: Debashish-PC
Binary: Built with gc go1.22.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0521 13:32:29.570185   13284 out.go:291] Setting OutFile to fd 104 ...
I0521 13:32:29.570185   13284 out.go:343] isatty.IsTerminal(104) = true
I0521 13:32:29.570185   13284 out.go:304] Setting ErrFile to fd 108...
I0521 13:32:29.570185   13284 out.go:343] isatty.IsTerminal(108) = true
I0521 13:32:29.594857   13284 out.go:298] Setting JSON to false
I0521 13:32:29.599429   13284 start.go:129] hostinfo: {"hostname":"Debashish-PC","uptime":513648,"bootTime":1715764900,"procs":301,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.22621.3593 Build 22621.3593","kernelVersion":"10.0.22621.3593 Build 22621.3593","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"3911431c-1969-4f64-8569-4078c8cdedfc"}
W0521 13:32:29.599429   13284 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0521 13:32:29.600939   13284 out.go:177] 😄  minikube v1.33.1 on Microsoft Windows 11 Home Single Language 10.0.22621.3593 Build 22621.3593
I0521 13:32:29.603149   13284 notify.go:220] Checking for updates...
I0521 13:32:29.603300   13284 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0521 13:32:29.603859   13284 driver.go:392] Setting default libvirt URI to qemu:///system
I0521 13:32:29.913365   13284 docker.go:122] docker version: linux-26.1.1:Docker Desktop 4.30.0 (149282)
I0521 13:32:29.915665   13284 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0521 13:32:30.551502   13284 info.go:266] docker info: {ID:dac81649-8078-4acc-900e-54faff1690ee Containers:21 ContainersRunning:21 ContainersPaused:0 ContainersStopped:0 Images:16 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:264 OomKillDisable:true NGoroutines:256 SystemTime:2024-05-21 08:02:30.508380925 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:4013940736 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0521 13:32:30.553253   13284 out.go:177] ✨  Using the docker driver based on existing profile
I0521 13:32:30.554344   13284 start.go:297] selected driver: docker
I0521 13:32:30.554344   13284 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\User:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0521 13:32:30.554344   13284 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0521 13:32:30.558563   13284 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0521 13:32:31.043275   13284 info.go:266] docker info: {ID:dac81649-8078-4acc-900e-54faff1690ee Containers:21 ContainersRunning:21 ContainersPaused:0 ContainersStopped:0 Images:16 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:264 OomKillDisable:true NGoroutines:256 SystemTime:2024-05-21 08:02:31.003393712 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:4013940736 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0521 13:32:31.086492   13284 cni.go:84] Creating CNI manager for ""
I0521 13:32:31.086492   13284 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0521 13:32:31.086492   13284 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\User:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0521 13:32:31.087735   13284 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0521 13:32:31.088347   13284 cache.go:121] Beginning downloading kic base image for docker with docker
I0521 13:32:31.088881   13284 out.go:177] 🚜  Pulling base image v0.0.44 ...
I0521 13:32:31.089994   13284 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0521 13:32:31.089994   13284 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0521 13:32:31.089994   13284 preload.go:147] Found local preload: C:\Users\User\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0521 13:32:31.089994   13284 cache.go:56] Caching tarball of preloaded images
I0521 13:32:31.090499   13284 preload.go:173] Found C:\Users\User\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0521 13:32:31.090499   13284 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0521 13:32:31.090499   13284 profile.go:143] Saving config to C:\Users\User\.minikube\profiles\minikube\config.json ...
I0521 13:32:31.381484   13284 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon, skipping pull
I0521 13:32:31.381484   13284 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in daemon, skipping load
I0521 13:32:31.381484   13284 cache.go:194] Successfully downloaded all kic artifacts
I0521 13:32:31.381988   13284 start.go:360] acquireMachinesLock for minikube: {Name:mk910e2d185d973fbe12ec8c7330b2d1d5ea153f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0521 13:32:31.381988   13284 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0521 13:32:31.381988   13284 start.go:96] Skipping create...Using existing machine configuration
I0521 13:32:31.381988   13284 fix.go:54] fixHost starting: 
I0521 13:32:31.386461   13284 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0521 13:32:31.675666   13284 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0521 13:32:31.675666   13284 fix.go:138] unexpected machine state, will restart: <nil>
I0521 13:32:31.676712   13284 out.go:177] 🏃  Updating the running docker "minikube" container ...
I0521 13:32:31.677328   13284 machine.go:94] provisionDockerMachine start ...
I0521 13:32:31.679483   13284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0521 13:32:31.998633   13284 main.go:141] libmachine: Using SSH client type: native
I0521 13:32:31.999822   13284 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x84a3c0] 0x84cfa0 <nil>  [] 0s} 127.0.0.1 63476 <nil> <nil>}
I0521 13:32:31.999822   13284 main.go:141] libmachine: About to run SSH command:
hostname
I0521 13:32:32.163464   13284 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0521 13:32:32.163533   13284 ubuntu.go:169] provisioning hostname "minikube"
I0521 13:32:32.166544   13284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0521 13:32:32.496776   13284 main.go:141] libmachine: Using SSH client type: native
I0521 13:32:32.497456   13284 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x84a3c0] 0x84cfa0 <nil>  [] 0s} 127.0.0.1 63476 <nil> <nil>}
I0521 13:32:32.497456   13284 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0521 13:32:32.662543   13284 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0521 13:32:32.664805   13284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0521 13:32:33.005856   13284 main.go:141] libmachine: Using SSH client type: native
I0521 13:32:33.005856   13284 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x84a3c0] 0x84cfa0 <nil>  [] 0s} 127.0.0.1 63476 <nil> <nil>}
I0521 13:32:33.005856   13284 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0521 13:32:33.176708   13284 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0521 13:32:33.176708   13284 ubuntu.go:175] set auth options {CertDir:C:\Users\User\.minikube CaCertPath:C:\Users\User\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\User\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\User\.minikube\machines\server.pem ServerKeyPath:C:\Users\User\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\User\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\User\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\User\.minikube}
I0521 13:32:33.177212   13284 ubuntu.go:177] setting up certificates
I0521 13:32:33.177212   13284 provision.go:84] configureAuth start
I0521 13:32:33.180125   13284 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0521 13:32:33.519789   13284 provision.go:143] copyHostCerts
I0521 13:32:33.520299   13284 exec_runner.go:144] found C:\Users\User\.minikube/cert.pem, removing ...
I0521 13:32:33.520299   13284 exec_runner.go:203] rm: C:\Users\User\.minikube\cert.pem
I0521 13:32:33.520362   13284 exec_runner.go:151] cp: C:\Users\User\.minikube\certs\cert.pem --> C:\Users\User\.minikube/cert.pem (1115 bytes)
I0521 13:32:33.521687   13284 exec_runner.go:144] found C:\Users\User\.minikube/key.pem, removing ...
I0521 13:32:33.521687   13284 exec_runner.go:203] rm: C:\Users\User\.minikube\key.pem
I0521 13:32:33.521687   13284 exec_runner.go:151] cp: C:\Users\User\.minikube\certs\key.pem --> C:\Users\User\.minikube/key.pem (1679 bytes)
I0521 13:32:33.522410   13284 exec_runner.go:144] found C:\Users\User\.minikube/ca.pem, removing ...
I0521 13:32:33.522410   13284 exec_runner.go:203] rm: C:\Users\User\.minikube\ca.pem
I0521 13:32:33.522917   13284 exec_runner.go:151] cp: C:\Users\User\.minikube\certs\ca.pem --> C:\Users\User\.minikube/ca.pem (1070 bytes)
I0521 13:32:33.523556   13284 provision.go:117] generating server cert: C:\Users\User\.minikube\machines\server.pem ca-key=C:\Users\User\.minikube\certs\ca.pem private-key=C:\Users\User\.minikube\certs\ca-key.pem org=User.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0521 13:32:33.668617   13284 provision.go:177] copyRemoteCerts
I0521 13:32:33.670265   13284 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0521 13:32:33.670265   13284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0521 13:32:34.018064   13284 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63476 SSHKeyPath:C:\Users\User\.minikube\machines\minikube\id_rsa Username:docker}
I0521 13:32:34.138924   13284 ssh_runner.go:362] scp C:\Users\User\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0521 13:32:34.168646   13284 ssh_runner.go:362] scp C:\Users\User\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0521 13:32:34.196792   13284 ssh_runner.go:362] scp C:\Users\User\.minikube\machines\server.pem --> /etc/docker/server.pem (1172 bytes)
I0521 13:32:34.228178   13284 provision.go:87] duration metric: took 1.0509657s to configureAuth
I0521 13:32:34.228178   13284 ubuntu.go:193] setting minikube options for container-runtime
I0521 13:32:34.228251   13284 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0521 13:32:34.231383   13284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0521 13:32:34.569703   13284 main.go:141] libmachine: Using SSH client type: native
I0521 13:32:34.570320   13284 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x84a3c0] 0x84cfa0 <nil>  [] 0s} 127.0.0.1 63476 <nil> <nil>}
I0521 13:32:34.570320   13284 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0521 13:32:34.736257   13284 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0521 13:32:34.736257   13284 ubuntu.go:71] root file system type: overlay
I0521 13:32:34.736257   13284 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0521 13:32:34.739291   13284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0521 13:32:35.073895   13284 main.go:141] libmachine: Using SSH client type: native
I0521 13:32:35.073895   13284 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x84a3c0] 0x84cfa0 <nil>  [] 0s} 127.0.0.1 63476 <nil> <nil>}
I0521 13:32:35.074495   13284 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0521 13:32:35.243687   13284 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0521 13:32:35.247593   13284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0521 13:32:35.582771   13284 main.go:141] libmachine: Using SSH client type: native
I0521 13:32:35.582771   13284 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x84a3c0] 0x84cfa0 <nil>  [] 0s} 127.0.0.1 63476 <nil> <nil>}
I0521 13:32:35.582771   13284 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0521 13:32:35.751782   13284 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0521 13:32:35.751782   13284 machine.go:97] duration metric: took 4.0744532s to provisionDockerMachine
I0521 13:32:35.751782   13284 start.go:293] postStartSetup for "minikube" (driver="docker")
I0521 13:32:35.751782   13284 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0521 13:32:35.761558   13284 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0521 13:32:35.765410   13284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0521 13:32:36.115223   13284 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63476 SSHKeyPath:C:\Users\User\.minikube\machines\minikube\id_rsa Username:docker}
I0521 13:32:36.234602   13284 ssh_runner.go:195] Run: cat /etc/os-release
I0521 13:32:36.241795   13284 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0521 13:32:36.241795   13284 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0521 13:32:36.241795   13284 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0521 13:32:36.241795   13284 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0521 13:32:36.241795   13284 filesync.go:126] Scanning C:\Users\User\.minikube\addons for local assets ...
I0521 13:32:36.242301   13284 filesync.go:126] Scanning C:\Users\User\.minikube\files for local assets ...
I0521 13:32:36.242368   13284 start.go:296] duration metric: took 490.5866ms for postStartSetup
I0521 13:32:36.248240   13284 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0521 13:32:36.250505   13284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0521 13:32:36.569467   13284 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63476 SSHKeyPath:C:\Users\User\.minikube\machines\minikube\id_rsa Username:docker}
I0521 13:32:36.679730   13284 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0521 13:32:36.687814   13284 fix.go:56] duration metric: took 5.3058259s for fixHost
I0521 13:32:36.687814   13284 start.go:83] releasing machines lock for "minikube", held for 5.3058259s
I0521 13:32:36.691688   13284 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0521 13:32:37.023719   13284 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0521 13:32:37.028395   13284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0521 13:32:37.033562   13284 ssh_runner.go:195] Run: cat /version.json
I0521 13:32:37.036291   13284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0521 13:32:37.373071   13284 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63476 SSHKeyPath:C:\Users\User\.minikube\machines\minikube\id_rsa Username:docker}
I0521 13:32:37.389363   13284 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63476 SSHKeyPath:C:\Users\User\.minikube\machines\minikube\id_rsa Username:docker}
I0521 13:32:37.489783   13284 ssh_runner.go:195] Run: systemctl --version
I0521 13:32:37.806491   13284 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0521 13:32:37.821589   13284 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0521 13:32:37.836241   13284 start.go:438] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0521 13:32:37.842030   13284 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0521 13:32:37.856115   13284 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0521 13:32:37.856115   13284 start.go:494] detecting cgroup driver to use...
I0521 13:32:37.856115   13284 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0521 13:32:37.856627   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0521 13:32:37.883596   13284 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0521 13:32:37.907543   13284 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0521 13:32:37.926636   13284 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0521 13:32:37.933109   13284 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0521 13:32:37.954983   13284 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0521 13:32:37.978014   13284 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0521 13:32:38.003158   13284 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0521 13:32:38.024950   13284 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0521 13:32:38.046817   13284 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0521 13:32:38.070263   13284 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0521 13:32:38.092115   13284 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0521 13:32:38.114883   13284 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0521 13:32:38.137642   13284 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0521 13:32:38.158926   13284 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0521 13:32:38.316560   13284 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0521 13:32:38.693944   13284 start.go:494] detecting cgroup driver to use...
I0521 13:32:38.693944   13284 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0521 13:32:38.703381   13284 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0521 13:32:38.725527   13284 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0521 13:32:38.734343   13284 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0521 13:32:38.760039   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0521 13:32:38.801169   13284 ssh_runner.go:195] Run: which cri-dockerd
I0521 13:32:38.849498   13284 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0521 13:32:38.869493   13284 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0521 13:32:38.909197   13284 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0521 13:32:39.084503   13284 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0521 13:32:39.270412   13284 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0521 13:32:39.270412   13284 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0521 13:32:39.310189   13284 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0521 13:32:39.502439   13284 ssh_runner.go:195] Run: sudo systemctl restart docker
I0521 13:32:40.617020   13284 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.1145806s)
I0521 13:32:40.629333   13284 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0521 13:32:40.657625   13284 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0521 13:32:40.712183   13284 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0521 13:32:40.740711   13284 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0521 13:32:40.871077   13284 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0521 13:32:41.006361   13284 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0521 13:32:41.134611   13284 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0521 13:32:41.166376   13284 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0521 13:32:41.193243   13284 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0521 13:32:41.319537   13284 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0521 13:32:41.547732   13284 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0521 13:32:41.557730   13284 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0521 13:32:41.567144   13284 start.go:562] Will wait 60s for crictl version
I0521 13:32:41.581452   13284 ssh_runner.go:195] Run: which crictl
I0521 13:32:41.599261   13284 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0521 13:32:41.663875   13284 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0521 13:32:41.670853   13284 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0521 13:32:41.713876   13284 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0521 13:32:41.759550   13284 out.go:204] 🐳  Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0521 13:32:41.763934   13284 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0521 13:32:42.278712   13284 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0521 13:32:42.286989   13284 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0521 13:32:42.299456   13284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0521 13:32:42.656397   13284 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\User:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0521 13:32:42.656397   13284 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0521 13:32:42.659863   13284 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0521 13:32:42.696229   13284 docker.go:685] Got preloaded images: -- stdout --
<none>:<none>
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0521 13:32:42.696229   13284 docker.go:615] Images already preloaded, skipping extraction
I0521 13:32:42.699531   13284 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0521 13:32:42.735412   13284 docker.go:685] Got preloaded images: -- stdout --
<none>:<none>
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0521 13:32:42.735412   13284 cache_images.go:84] Images are preloaded, skipping loading
I0521 13:32:42.735412   13284 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0521 13:32:42.735412   13284 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0521 13:32:42.739048   13284 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0521 13:32:43.009703   13284 cni.go:84] Creating CNI manager for ""
I0521 13:32:43.009703   13284 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0521 13:32:43.009703   13284 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0521 13:32:43.009703   13284 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0521 13:32:43.010213   13284 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0521 13:32:43.034590   13284 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0521 13:32:43.060809   13284 binaries.go:44] Found k8s binaries, skipping transfer
I0521 13:32:43.088409   13284 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0521 13:32:43.109732   13284 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0521 13:32:43.149372   13284 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0521 13:32:43.192374   13284 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0521 13:32:43.239746   13284 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0521 13:32:43.259633   13284 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0521 13:32:43.414760   13284 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0521 13:32:43.447113   13284 certs.go:68] Setting up C:\Users\User\.minikube\profiles\minikube for IP: 192.168.49.2
I0521 13:32:43.447711   13284 certs.go:194] generating shared ca certs ...
I0521 13:32:43.447711   13284 certs.go:226] acquiring lock for ca certs: {Name:mk0fa5de13b5935dba1bd738b050f4c2e4c5789d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0521 13:32:43.448832   13284 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\User\.minikube\ca.key
I0521 13:32:43.449937   13284 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\User\.minikube\proxy-client-ca.key
I0521 13:32:43.450494   13284 certs.go:256] generating profile certs ...
I0521 13:32:43.451665   13284 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\User\.minikube\profiles\minikube\client.key
I0521 13:32:43.452758   13284 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\User\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0521 13:32:43.453365   13284 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\User\.minikube\profiles\minikube\proxy-client.key
I0521 13:32:43.455755   13284 certs.go:484] found cert: C:\Users\User\.minikube\certs\ca-key.pem (1675 bytes)
I0521 13:32:43.455836   13284 certs.go:484] found cert: C:\Users\User\.minikube\certs\ca.pem (1070 bytes)
I0521 13:32:43.456351   13284 certs.go:484] found cert: C:\Users\User\.minikube\certs\cert.pem (1115 bytes)
I0521 13:32:43.456552   13284 certs.go:484] found cert: C:\Users\User\.minikube\certs\key.pem (1679 bytes)
I0521 13:32:43.458279   13284 ssh_runner.go:362] scp C:\Users\User\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0521 13:32:43.534864   13284 ssh_runner.go:362] scp C:\Users\User\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0521 13:32:43.580518   13284 ssh_runner.go:362] scp C:\Users\User\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0521 13:32:43.641063   13284 ssh_runner.go:362] scp C:\Users\User\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0521 13:32:43.686610   13284 ssh_runner.go:362] scp C:\Users\User\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0521 13:32:43.733856   13284 ssh_runner.go:362] scp C:\Users\User\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0521 13:32:43.774541   13284 ssh_runner.go:362] scp C:\Users\User\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0521 13:32:43.819766   13284 ssh_runner.go:362] scp C:\Users\User\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0521 13:32:43.859127   13284 ssh_runner.go:362] scp C:\Users\User\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0521 13:32:43.898620   13284 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0521 13:32:43.937498   13284 ssh_runner.go:195] Run: openssl version
I0521 13:32:43.961229   13284 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0521 13:32:43.989242   13284 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0521 13:32:43.997745   13284 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May 16 09:58 /usr/share/ca-certificates/minikubeCA.pem
I0521 13:32:44.006375   13284 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0521 13:32:44.033079   13284 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0521 13:32:44.059485   13284 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0521 13:32:44.076692   13284 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0521 13:32:44.097733   13284 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0521 13:32:44.120144   13284 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0521 13:32:44.143210   13284 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0521 13:32:44.167460   13284 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0521 13:32:44.189726   13284 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0521 13:32:44.203142   13284 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\User:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0521 13:32:44.207353   13284 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0521 13:32:44.246321   13284 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0521 13:32:44.263893   13284 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0521 13:32:44.263893   13284 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0521 13:32:44.263893   13284 kubeadm.go:587] restartPrimaryControlPlane start ...
I0521 13:32:44.272655   13284 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0521 13:32:44.286016   13284 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0521 13:32:44.289974   13284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0521 13:32:44.652393   13284 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:63475"
I0521 13:32:44.664717   13284 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0521 13:32:44.682528   13284 kubeadm.go:624] The running cluster does not require reconfiguration: 127.0.0.1
I0521 13:32:44.682528   13284 kubeadm.go:591] duration metric: took 418.6348ms to restartPrimaryControlPlane
I0521 13:32:44.682528   13284 kubeadm.go:393] duration metric: took 479.3862ms to StartCluster
I0521 13:32:44.682528   13284 settings.go:142] acquiring lock: {Name:mkbb158d68fd483b3bca5b8a92ada72bb65452d7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0521 13:32:44.682528   13284 settings.go:150] Updating kubeconfig:  C:\Users\User\.kube\config
I0521 13:32:44.684787   13284 lock.go:35] WriteFile acquiring C:\Users\User\.kube\config: {Name:mk4c22283734d3c4206a5f8b8d3853863fcc1163 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0521 13:32:44.685496   13284 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0521 13:32:44.686787   13284 out.go:177] 🔎  Verifying Kubernetes components...
I0521 13:32:44.685496   13284 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0521 13:32:44.686787   13284 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0521 13:32:44.686002   13284 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0521 13:32:44.686787   13284 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0521 13:32:44.687074   13284 addons.go:243] addon storage-provisioner should already be in state true
I0521 13:32:44.687074   13284 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0521 13:32:44.687074   13284 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0521 13:32:44.687668   13284 host.go:66] Checking if "minikube" exists ...
I0521 13:32:44.696743   13284 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0521 13:32:44.698510   13284 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0521 13:32:44.703472   13284 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0521 13:32:44.849436   13284 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0521 13:32:45.103344   13284 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0521 13:32:45.103344   13284 addons.go:243] addon default-storageclass should already be in state true
I0521 13:32:45.103344   13284 host.go:66] Checking if "minikube" exists ...
I0521 13:32:45.112069   13284 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0521 13:32:45.117601   13284 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0521 13:32:45.118738   13284 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0521 13:32:45.118738   13284 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0521 13:32:45.122999   13284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0521 13:32:45.199308   13284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0521 13:32:45.564845   13284 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0521 13:32:45.565395   13284 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0521 13:32:45.576040   13284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0521 13:32:45.579888   13284 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63476 SSHKeyPath:C:\Users\User\.minikube\machines\minikube\id_rsa Username:docker}
I0521 13:32:45.672063   13284 api_server.go:52] waiting for apiserver process to appear ...
I0521 13:32:45.682903   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:45.747903   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
W0521 13:32:45.937635   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:45.937635   13284 retry.go:31] will retry after 130.551141ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:46.082451   13284 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63476 SSHKeyPath:C:\Users\User\.minikube\machines\minikube\id_rsa Username:docker}
I0521 13:32:46.096215   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0521 13:32:46.205273   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0521 13:32:46.364949   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:46.364949   13284 retry.go:31] will retry after 294.239073ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:46.374918   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W0521 13:32:46.592865   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:46.592865   13284 retry.go:31] will retry after 327.018612ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:46.688173   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0521 13:32:46.688872   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0521 13:32:46.811703   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:46.811810   13284 retry.go:31] will retry after 622.092377ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:46.944383   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0521 13:32:47.045867   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:47.045867   13284 retry.go:31] will retry after 386.746231ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:47.194015   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:47.448267   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0521 13:32:47.449031   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0521 13:32:47.532982   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0521 13:32:47.532982   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:47.532982   13284 retry.go:31] will retry after 1.202673221s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:47.532982   13284 retry.go:31] will retry after 361.484883ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:47.697990   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:47.918944   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0521 13:32:47.986092   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:47.986092   13284 retry.go:31] will retry after 444.479033ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:48.185327   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:48.451191   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0521 13:32:48.506332   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:48.506332   13284 retry.go:31] will retry after 1.176231941s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:48.687826   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:48.744105   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0521 13:32:48.797886   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:48.797886   13284 retry.go:31] will retry after 642.925326ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:49.186620   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:49.453403   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0521 13:32:49.534196   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:49.534196   13284 retry.go:31] will retry after 2.045248187s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:49.695212   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0521 13:32:49.695212   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0521 13:32:49.750331   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:49.750331   13284 retry.go:31] will retry after 1.697767703s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:50.195362   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:50.680952   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:51.180815   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:51.465523   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0521 13:32:51.517865   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:51.517865   13284 retry.go:31] will retry after 3.146964495s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:51.589787   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0521 13:32:51.642458   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:51.642458   13284 retry.go:31] will retry after 3.544111617s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:51.690008   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:52.191295   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:52.686565   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:53.185168   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:53.688545   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:54.187219   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:54.671760   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0521 13:32:54.676497   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0521 13:32:54.760998   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:54.761183   13284 retry.go:31] will retry after 3.121021804s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:55.181372   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:55.192931   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0521 13:32:55.259475   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:55.259475   13284 retry.go:31] will retry after 3.665362765s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:55.694126   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:56.177936   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:56.692362   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:57.186311   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:57.694597   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:57.887934   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0521 13:32:57.948041   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:57.948041   13284 retry.go:31] will retry after 6.694714155s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:58.185530   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:58.685587   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:58.938844   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0521 13:32:58.994538   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:58.994538   13284 retry.go:31] will retry after 6.087698265s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:32:59.187385   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:32:59.683452   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:00.183858   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:00.687032   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:01.187681   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:01.692554   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:02.194970   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:02.694095   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:03.177676   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:03.680024   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:04.183194   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:04.670431   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0521 13:33:04.697544   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0521 13:33:04.766001   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:33:04.766001   13284 retry.go:31] will retry after 11.493863547s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:33:05.108719   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0521 13:33:05.191771   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0521 13:33:05.208108   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:33:05.208108   13284 retry.go:31] will retry after 13.527535363s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:33:05.696047   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:06.193612   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:06.695231   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:07.200048   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:07.704483   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:08.205208   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:08.689745   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:09.194056   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:09.695566   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:10.195522   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:10.697520   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:11.206136   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:11.699056   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:12.187691   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:12.681811   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:13.183823   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:13.687570   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:14.181236   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:14.685909   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:15.189590   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:15.691800   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:16.185538   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:16.264850   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0521 13:33:16.324338   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:33:16.324338   13284 retry.go:31] will retry after 11.445075915s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:33:16.681060   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:17.191954   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:17.680713   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:18.190450   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:18.694492   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:18.749173   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0521 13:33:18.817097   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:33:18.817097   13284 retry.go:31] will retry after 11.189150106s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:33:19.182128   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:19.692791   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:20.190373   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:20.697579   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:21.183564   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:21.679294   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:22.188205   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:22.709810   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:23.208036   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:23.685871   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:24.182986   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:24.686031   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:25.184320   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:25.696378   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:26.189018   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:26.683246   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:27.186290   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:27.687149   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:27.791353   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0521 13:33:27.938866   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:33:27.938866   13284 retry.go:31] will retry after 27.204886092s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:33:28.199319   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:28.702037   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:29.195607   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:29.689856   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:30.025995   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0521 13:33:30.097651   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:33:30.097651   13284 retry.go:31] will retry after 24.954198968s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:33:30.194182   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:30.701880   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:31.196510   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:31.689536   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:32.195802   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:32.687233   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:33.197897   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:33.686196   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:34.189674   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:34.705628   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:35.195418   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:35.696857   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:36.204412   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:36.694657   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:37.198878   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:37.692141   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:38.209431   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:38.698844   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:39.214419   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:39.702363   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:40.189904   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:40.704937   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:41.199465   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:41.690812   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:42.204369   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:42.702234   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:43.201126   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:43.704865   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:44.192607   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:44.695371   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:45.188107   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0521 13:33:45.232169   13284 logs.go:276] 1 containers: [c1e0c79fbdfc]
I0521 13:33:45.240963   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0521 13:33:45.276782   13284 logs.go:276] 1 containers: [741730d728ce]
I0521 13:33:45.285709   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0521 13:33:45.324996   13284 logs.go:276] 2 containers: [2dcca18ff085 95f2456f0b22]
I0521 13:33:45.335289   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0521 13:33:45.366888   13284 logs.go:276] 1 containers: [f88690a5517b]
I0521 13:33:45.376134   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0521 13:33:45.409586   13284 logs.go:276] 1 containers: [e1858dc15ac9]
I0521 13:33:45.421793   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0521 13:33:45.459913   13284 logs.go:276] 1 containers: [c8f2c632c356]
I0521 13:33:45.469065   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0521 13:33:45.503368   13284 logs.go:276] 0 containers: []
W0521 13:33:45.503669   13284 logs.go:278] No container was found matching "kindnet"
I0521 13:33:45.513644   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0521 13:33:45.538260   13284 logs.go:276] 2 containers: [b279ab02cf1a bd82d4f75a25]
I0521 13:33:45.538764   13284 logs.go:123] Gathering logs for kube-scheduler [f88690a5517b] ...
I0521 13:33:45.538764   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f88690a5517b"
I0521 13:33:45.632243   13284 logs.go:123] Gathering logs for kube-controller-manager [c8f2c632c356] ...
I0521 13:33:45.632243   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c8f2c632c356"
I0521 13:33:45.709675   13284 logs.go:123] Gathering logs for storage-provisioner [bd82d4f75a25] ...
I0521 13:33:45.709675   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bd82d4f75a25"
I0521 13:33:45.743586   13284 logs.go:123] Gathering logs for etcd [741730d728ce] ...
I0521 13:33:45.743586   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 741730d728ce"
I0521 13:33:45.845776   13284 logs.go:123] Gathering logs for coredns [2dcca18ff085] ...
I0521 13:33:45.845776   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2dcca18ff085"
I0521 13:33:45.924159   13284 logs.go:123] Gathering logs for kube-apiserver [c1e0c79fbdfc] ...
I0521 13:33:45.924159   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c1e0c79fbdfc"
I0521 13:33:46.005585   13284 logs.go:123] Gathering logs for storage-provisioner [b279ab02cf1a] ...
I0521 13:33:46.005585   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b279ab02cf1a"
I0521 13:33:46.061127   13284 logs.go:123] Gathering logs for Docker ...
I0521 13:33:46.061127   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0521 13:33:46.134848   13284 logs.go:123] Gathering logs for kubelet ...
I0521 13:33:46.134848   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0521 13:33:46.197147   13284 logs.go:123] Gathering logs for dmesg ...
I0521 13:33:46.197147   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0521 13:33:46.213786   13284 logs.go:123] Gathering logs for kube-proxy [e1858dc15ac9] ...
I0521 13:33:46.213786   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1858dc15ac9"
I0521 13:33:46.263837   13284 logs.go:123] Gathering logs for container status ...
I0521 13:33:46.263837   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0521 13:33:46.335136   13284 logs.go:123] Gathering logs for describe nodes ...
I0521 13:33:46.335136   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0521 13:33:46.401292   13284 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0521 08:03:46.389477   48824 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:46.390208   48824 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:46.392127   48824 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:46.392686   48824 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:46.394392   48824 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0521 08:03:46.389477   48824 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:46.390208   48824 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:46.392127   48824 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:46.392686   48824 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:46.394392   48824 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0521 13:33:46.401292   13284 logs.go:123] Gathering logs for coredns [95f2456f0b22] ...
I0521 13:33:46.401292   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 95f2456f0b22"
I0521 13:33:49.010908   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:49.034604   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0521 13:33:49.055420   13284 logs.go:276] 1 containers: [c1e0c79fbdfc]
I0521 13:33:49.063789   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0521 13:33:49.084852   13284 logs.go:276] 1 containers: [741730d728ce]
I0521 13:33:49.093134   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0521 13:33:49.112523   13284 logs.go:276] 2 containers: [2dcca18ff085 95f2456f0b22]
I0521 13:33:49.119499   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0521 13:33:49.139334   13284 logs.go:276] 1 containers: [f88690a5517b]
I0521 13:33:49.147206   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0521 13:33:49.167229   13284 logs.go:276] 1 containers: [e1858dc15ac9]
I0521 13:33:49.174031   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0521 13:33:49.192914   13284 logs.go:276] 1 containers: [c8f2c632c356]
I0521 13:33:49.200159   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0521 13:33:49.217703   13284 logs.go:276] 0 containers: []
W0521 13:33:49.217703   13284 logs.go:278] No container was found matching "kindnet"
I0521 13:33:49.224624   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0521 13:33:49.243615   13284 logs.go:276] 2 containers: [b279ab02cf1a bd82d4f75a25]
I0521 13:33:49.243615   13284 logs.go:123] Gathering logs for etcd [741730d728ce] ...
I0521 13:33:49.243615   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 741730d728ce"
I0521 13:33:49.343703   13284 logs.go:123] Gathering logs for coredns [2dcca18ff085] ...
I0521 13:33:49.343703   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2dcca18ff085"
I0521 13:33:49.415227   13284 logs.go:123] Gathering logs for coredns [95f2456f0b22] ...
I0521 13:33:49.415227   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 95f2456f0b22"
I0521 13:33:49.510279   13284 logs.go:123] Gathering logs for kube-scheduler [f88690a5517b] ...
I0521 13:33:49.510279   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f88690a5517b"
I0521 13:33:49.588773   13284 logs.go:123] Gathering logs for kube-controller-manager [c8f2c632c356] ...
I0521 13:33:49.588773   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c8f2c632c356"
I0521 13:33:49.666187   13284 logs.go:123] Gathering logs for storage-provisioner [bd82d4f75a25] ...
I0521 13:33:49.666187   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bd82d4f75a25"
I0521 13:33:49.699098   13284 logs.go:123] Gathering logs for container status ...
I0521 13:33:49.699098   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0521 13:33:49.743493   13284 logs.go:123] Gathering logs for describe nodes ...
I0521 13:33:49.743493   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0521 13:33:49.804450   13284 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0521 08:03:49.794191   49046 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:49.794856   49046 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:49.796382   49046 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:49.796560   49046 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:49.798073   49046 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0521 08:03:49.794191   49046 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:49.794856   49046 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:49.796382   49046 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:49.796560   49046 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:49.798073   49046 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0521 13:33:49.804450   13284 logs.go:123] Gathering logs for kubelet ...
I0521 13:33:49.804450   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0521 13:33:49.855010   13284 logs.go:123] Gathering logs for dmesg ...
I0521 13:33:49.855010   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0521 13:33:49.878394   13284 logs.go:123] Gathering logs for kube-apiserver [c1e0c79fbdfc] ...
I0521 13:33:49.878394   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c1e0c79fbdfc"
I0521 13:33:49.940774   13284 logs.go:123] Gathering logs for kube-proxy [e1858dc15ac9] ...
I0521 13:33:49.940774   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1858dc15ac9"
I0521 13:33:49.990321   13284 logs.go:123] Gathering logs for storage-provisioner [b279ab02cf1a] ...
I0521 13:33:49.990321   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b279ab02cf1a"
I0521 13:33:50.032336   13284 logs.go:123] Gathering logs for Docker ...
I0521 13:33:50.032336   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0521 13:33:52.630080   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:52.646843   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0521 13:33:52.665591   13284 logs.go:276] 1 containers: [c1e0c79fbdfc]
I0521 13:33:52.673630   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0521 13:33:52.691691   13284 logs.go:276] 1 containers: [741730d728ce]
I0521 13:33:52.698837   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0521 13:33:52.719056   13284 logs.go:276] 2 containers: [2dcca18ff085 95f2456f0b22]
I0521 13:33:52.727118   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0521 13:33:52.748218   13284 logs.go:276] 1 containers: [f88690a5517b]
I0521 13:33:52.757029   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0521 13:33:52.778764   13284 logs.go:276] 1 containers: [e1858dc15ac9]
I0521 13:33:52.785624   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0521 13:33:52.804743   13284 logs.go:276] 1 containers: [c8f2c632c356]
I0521 13:33:52.811377   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0521 13:33:52.831148   13284 logs.go:276] 0 containers: []
W0521 13:33:52.831148   13284 logs.go:278] No container was found matching "kindnet"
I0521 13:33:52.838308   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0521 13:33:52.857605   13284 logs.go:276] 2 containers: [b279ab02cf1a bd82d4f75a25]
I0521 13:33:52.857605   13284 logs.go:123] Gathering logs for coredns [2dcca18ff085] ...
I0521 13:33:52.857605   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2dcca18ff085"
I0521 13:33:52.923069   13284 logs.go:123] Gathering logs for coredns [95f2456f0b22] ...
I0521 13:33:52.923069   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 95f2456f0b22"
I0521 13:33:52.999857   13284 logs.go:123] Gathering logs for kube-proxy [e1858dc15ac9] ...
I0521 13:33:52.999857   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1858dc15ac9"
I0521 13:33:53.051253   13284 logs.go:123] Gathering logs for kubelet ...
I0521 13:33:53.051253   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0521 13:33:53.107789   13284 logs.go:123] Gathering logs for dmesg ...
I0521 13:33:53.107789   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0521 13:33:53.126541   13284 logs.go:123] Gathering logs for describe nodes ...
I0521 13:33:53.126541   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0521 13:33:53.190537   13284 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0521 08:03:53.178830   49238 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:53.179889   49238 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:53.180903   49238 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:53.181489   49238 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:53.183402   49238 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0521 08:03:53.178830   49238 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:53.179889   49238 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:53.180903   49238 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:53.181489   49238 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:53.183402   49238 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0521 13:33:53.190537   13284 logs.go:123] Gathering logs for etcd [741730d728ce] ...
I0521 13:33:53.190537   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 741730d728ce"
I0521 13:33:53.297684   13284 logs.go:123] Gathering logs for Docker ...
I0521 13:33:53.297684   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0521 13:33:53.370561   13284 logs.go:123] Gathering logs for storage-provisioner [b279ab02cf1a] ...
I0521 13:33:53.370561   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b279ab02cf1a"
I0521 13:33:53.416978   13284 logs.go:123] Gathering logs for storage-provisioner [bd82d4f75a25] ...
I0521 13:33:53.416978   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bd82d4f75a25"
I0521 13:33:53.437841   13284 logs.go:123] Gathering logs for kube-apiserver [c1e0c79fbdfc] ...
I0521 13:33:53.437841   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c1e0c79fbdfc"
I0521 13:33:53.501307   13284 logs.go:123] Gathering logs for kube-scheduler [f88690a5517b] ...
I0521 13:33:53.501307   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f88690a5517b"
I0521 13:33:53.575679   13284 logs.go:123] Gathering logs for kube-controller-manager [c8f2c632c356] ...
I0521 13:33:53.575679   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c8f2c632c356"
I0521 13:33:53.639327   13284 logs.go:123] Gathering logs for container status ...
I0521 13:33:53.639327   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0521 13:33:55.076895   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0521 13:33:55.131609   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:33:55.131609   13284 retry.go:31] will retry after 29.582899525s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:33:55.164594   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0521 13:33:55.217786   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:33:55.217985   13284 retry.go:31] will retry after 33.648909588s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0521 13:33:56.201251   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:56.220396   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0521 13:33:56.247273   13284 logs.go:276] 1 containers: [c1e0c79fbdfc]
I0521 13:33:56.255056   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0521 13:33:56.274592   13284 logs.go:276] 1 containers: [741730d728ce]
I0521 13:33:56.282817   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0521 13:33:56.300924   13284 logs.go:276] 2 containers: [2dcca18ff085 95f2456f0b22]
I0521 13:33:56.308085   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0521 13:33:56.328135   13284 logs.go:276] 1 containers: [f88690a5517b]
I0521 13:33:56.335187   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0521 13:33:56.353722   13284 logs.go:276] 1 containers: [e1858dc15ac9]
I0521 13:33:56.360009   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0521 13:33:56.378688   13284 logs.go:276] 1 containers: [c8f2c632c356]
I0521 13:33:56.384115   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0521 13:33:56.404500   13284 logs.go:276] 0 containers: []
W0521 13:33:56.404500   13284 logs.go:278] No container was found matching "kindnet"
I0521 13:33:56.411298   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0521 13:33:56.431755   13284 logs.go:276] 2 containers: [b279ab02cf1a bd82d4f75a25]
I0521 13:33:56.431755   13284 logs.go:123] Gathering logs for etcd [741730d728ce] ...
I0521 13:33:56.431755   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 741730d728ce"
I0521 13:33:56.529579   13284 logs.go:123] Gathering logs for kube-controller-manager [c8f2c632c356] ...
I0521 13:33:56.529579   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c8f2c632c356"
I0521 13:33:56.586115   13284 logs.go:123] Gathering logs for kubelet ...
I0521 13:33:56.586115   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0521 13:33:56.649250   13284 logs.go:123] Gathering logs for describe nodes ...
I0521 13:33:56.649250   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0521 13:33:56.712383   13284 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0521 08:03:56.699955   49485 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:56.701279   49485 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:56.701459   49485 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:56.703468   49485 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:56.704268   49485 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0521 08:03:56.699955   49485 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:56.701279   49485 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:56.701459   49485 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:56.703468   49485 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:03:56.704268   49485 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0521 13:33:56.712383   13284 logs.go:123] Gathering logs for kube-proxy [e1858dc15ac9] ...
I0521 13:33:56.712383   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1858dc15ac9"
I0521 13:33:56.753915   13284 logs.go:123] Gathering logs for storage-provisioner [bd82d4f75a25] ...
I0521 13:33:56.753915   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bd82d4f75a25"
I0521 13:33:56.776055   13284 logs.go:123] Gathering logs for dmesg ...
I0521 13:33:56.776055   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0521 13:33:56.790203   13284 logs.go:123] Gathering logs for kube-apiserver [c1e0c79fbdfc] ...
I0521 13:33:56.790203   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c1e0c79fbdfc"
I0521 13:33:56.856202   13284 logs.go:123] Gathering logs for coredns [2dcca18ff085] ...
I0521 13:33:56.856202   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2dcca18ff085"
I0521 13:33:56.934424   13284 logs.go:123] Gathering logs for coredns [95f2456f0b22] ...
I0521 13:33:56.934424   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 95f2456f0b22"
I0521 13:33:57.018645   13284 logs.go:123] Gathering logs for kube-scheduler [f88690a5517b] ...
I0521 13:33:57.018645   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f88690a5517b"
I0521 13:33:57.091592   13284 logs.go:123] Gathering logs for storage-provisioner [b279ab02cf1a] ...
I0521 13:33:57.091592   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b279ab02cf1a"
I0521 13:33:57.138862   13284 logs.go:123] Gathering logs for Docker ...
I0521 13:33:57.138862   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0521 13:33:57.209623   13284 logs.go:123] Gathering logs for container status ...
I0521 13:33:57.209623   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0521 13:33:59.769093   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:33:59.791528   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0521 13:33:59.811199   13284 logs.go:276] 1 containers: [c1e0c79fbdfc]
I0521 13:33:59.819361   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0521 13:33:59.839597   13284 logs.go:276] 1 containers: [741730d728ce]
I0521 13:33:59.847102   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0521 13:33:59.865869   13284 logs.go:276] 2 containers: [2dcca18ff085 95f2456f0b22]
I0521 13:33:59.875469   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0521 13:33:59.894806   13284 logs.go:276] 1 containers: [f88690a5517b]
I0521 13:33:59.902594   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0521 13:33:59.920766   13284 logs.go:276] 1 containers: [e1858dc15ac9]
I0521 13:33:59.928045   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0521 13:33:59.953911   13284 logs.go:276] 1 containers: [c8f2c632c356]
I0521 13:33:59.960825   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0521 13:33:59.993202   13284 logs.go:276] 0 containers: []
W0521 13:33:59.993202   13284 logs.go:278] No container was found matching "kindnet"
I0521 13:33:59.999588   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0521 13:34:00.024022   13284 logs.go:276] 2 containers: [b279ab02cf1a bd82d4f75a25]
I0521 13:34:00.024022   13284 logs.go:123] Gathering logs for kube-proxy [e1858dc15ac9] ...
I0521 13:34:00.024022   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1858dc15ac9"
I0521 13:34:00.076568   13284 logs.go:123] Gathering logs for kube-controller-manager [c8f2c632c356] ...
I0521 13:34:00.076568   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c8f2c632c356"
I0521 13:34:00.145888   13284 logs.go:123] Gathering logs for storage-provisioner [bd82d4f75a25] ...
I0521 13:34:00.145888   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bd82d4f75a25"
I0521 13:34:00.170179   13284 logs.go:123] Gathering logs for storage-provisioner [b279ab02cf1a] ...
I0521 13:34:00.170179   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b279ab02cf1a"
I0521 13:34:00.212364   13284 logs.go:123] Gathering logs for kubelet ...
I0521 13:34:00.212499   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0521 13:34:00.264421   13284 logs.go:123] Gathering logs for etcd [741730d728ce] ...
I0521 13:34:00.264421   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 741730d728ce"
I0521 13:34:00.353460   13284 logs.go:123] Gathering logs for kube-scheduler [f88690a5517b] ...
I0521 13:34:00.353460   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f88690a5517b"
I0521 13:34:00.421490   13284 logs.go:123] Gathering logs for container status ...
I0521 13:34:00.421490   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0521 13:34:00.464815   13284 logs.go:123] Gathering logs for dmesg ...
I0521 13:34:00.464815   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0521 13:34:00.478424   13284 logs.go:123] Gathering logs for kube-apiserver [c1e0c79fbdfc] ...
I0521 13:34:00.478424   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c1e0c79fbdfc"
I0521 13:34:00.533460   13284 logs.go:123] Gathering logs for coredns [2dcca18ff085] ...
I0521 13:34:00.533460   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2dcca18ff085"
I0521 13:34:00.611530   13284 logs.go:123] Gathering logs for describe nodes ...
I0521 13:34:00.611530   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0521 13:34:00.667498   13284 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0521 08:04:00.658446   49786 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:00.658940   49786 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:00.660606   49786 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:00.660836   49786 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:00.662520   49786 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0521 08:04:00.658446   49786 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:00.658940   49786 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:00.660606   49786 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:00.660836   49786 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:00.662520   49786 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0521 13:34:00.667498   13284 logs.go:123] Gathering logs for coredns [95f2456f0b22] ...
I0521 13:34:00.667498   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 95f2456f0b22"
I0521 13:34:00.737219   13284 logs.go:123] Gathering logs for Docker ...
I0521 13:34:00.737219   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0521 13:34:03.330186   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:34:03.348684   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0521 13:34:03.370610   13284 logs.go:276] 1 containers: [c1e0c79fbdfc]
I0521 13:34:03.377963   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0521 13:34:03.397802   13284 logs.go:276] 1 containers: [741730d728ce]
I0521 13:34:03.404932   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0521 13:34:03.423146   13284 logs.go:276] 2 containers: [2dcca18ff085 95f2456f0b22]
I0521 13:34:03.430203   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0521 13:34:03.448929   13284 logs.go:276] 1 containers: [f88690a5517b]
I0521 13:34:03.455990   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0521 13:34:03.475645   13284 logs.go:276] 1 containers: [e1858dc15ac9]
I0521 13:34:03.482251   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0521 13:34:03.500075   13284 logs.go:276] 1 containers: [c8f2c632c356]
I0521 13:34:03.507132   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0521 13:34:03.525432   13284 logs.go:276] 0 containers: []
W0521 13:34:03.525432   13284 logs.go:278] No container was found matching "kindnet"
I0521 13:34:03.531952   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0521 13:34:03.549190   13284 logs.go:276] 2 containers: [b279ab02cf1a bd82d4f75a25]
I0521 13:34:03.549190   13284 logs.go:123] Gathering logs for kubelet ...
I0521 13:34:03.549190   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0521 13:34:03.595782   13284 logs.go:123] Gathering logs for dmesg ...
I0521 13:34:03.595782   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0521 13:34:03.619427   13284 logs.go:123] Gathering logs for kube-apiserver [c1e0c79fbdfc] ...
I0521 13:34:03.619427   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c1e0c79fbdfc"
I0521 13:34:03.686019   13284 logs.go:123] Gathering logs for kube-controller-manager [c8f2c632c356] ...
I0521 13:34:03.686019   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c8f2c632c356"
I0521 13:34:03.764808   13284 logs.go:123] Gathering logs for kube-scheduler [f88690a5517b] ...
I0521 13:34:03.764808   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f88690a5517b"
I0521 13:34:03.843174   13284 logs.go:123] Gathering logs for storage-provisioner [bd82d4f75a25] ...
I0521 13:34:03.843174   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bd82d4f75a25"
I0521 13:34:03.867912   13284 logs.go:123] Gathering logs for describe nodes ...
I0521 13:34:03.867912   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0521 13:34:03.923114   13284 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0521 08:04:03.911144   49982 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:03.911796   49982 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:03.913093   49982 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:03.913691   49982 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:03.915799   49982 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0521 08:04:03.911144   49982 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:03.911796   49982 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:03.913093   49982 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:03.913691   49982 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:03.915799   49982 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0521 13:34:03.923114   13284 logs.go:123] Gathering logs for etcd [741730d728ce] ...
I0521 13:34:03.923114   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 741730d728ce"
I0521 13:34:04.018331   13284 logs.go:123] Gathering logs for kube-proxy [e1858dc15ac9] ...
I0521 13:34:04.018331   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1858dc15ac9"
I0521 13:34:04.073242   13284 logs.go:123] Gathering logs for storage-provisioner [b279ab02cf1a] ...
I0521 13:34:04.073242   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b279ab02cf1a"
I0521 13:34:04.116520   13284 logs.go:123] Gathering logs for coredns [2dcca18ff085] ...
I0521 13:34:04.116520   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2dcca18ff085"
I0521 13:34:04.187250   13284 logs.go:123] Gathering logs for coredns [95f2456f0b22] ...
I0521 13:34:04.187250   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 95f2456f0b22"
I0521 13:34:04.255694   13284 logs.go:123] Gathering logs for Docker ...
I0521 13:34:04.255694   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0521 13:34:04.320178   13284 logs.go:123] Gathering logs for container status ...
I0521 13:34:04.320178   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0521 13:34:06.894702   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:34:06.924788   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0521 13:34:06.962943   13284 logs.go:276] 1 containers: [c1e0c79fbdfc]
I0521 13:34:06.974135   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0521 13:34:07.000226   13284 logs.go:276] 1 containers: [741730d728ce]
I0521 13:34:07.008133   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0521 13:34:07.031351   13284 logs.go:276] 2 containers: [2dcca18ff085 95f2456f0b22]
I0521 13:34:07.039559   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0521 13:34:07.059506   13284 logs.go:276] 1 containers: [f88690a5517b]
I0521 13:34:07.065967   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0521 13:34:07.088408   13284 logs.go:276] 1 containers: [e1858dc15ac9]
I0521 13:34:07.098021   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0521 13:34:07.119633   13284 logs.go:276] 1 containers: [c8f2c632c356]
I0521 13:34:07.128753   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0521 13:34:07.149558   13284 logs.go:276] 0 containers: []
W0521 13:34:07.149558   13284 logs.go:278] No container was found matching "kindnet"
I0521 13:34:07.157281   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0521 13:34:07.175233   13284 logs.go:276] 2 containers: [b279ab02cf1a bd82d4f75a25]
I0521 13:34:07.175742   13284 logs.go:123] Gathering logs for coredns [95f2456f0b22] ...
I0521 13:34:07.175742   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 95f2456f0b22"
I0521 13:34:07.240926   13284 logs.go:123] Gathering logs for storage-provisioner [bd82d4f75a25] ...
I0521 13:34:07.240926   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bd82d4f75a25"
I0521 13:34:07.266783   13284 logs.go:123] Gathering logs for Docker ...
I0521 13:34:07.266783   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0521 13:34:07.330809   13284 logs.go:123] Gathering logs for storage-provisioner [b279ab02cf1a] ...
I0521 13:34:07.330809   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b279ab02cf1a"
I0521 13:34:07.375765   13284 logs.go:123] Gathering logs for describe nodes ...
I0521 13:34:07.375765   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0521 13:34:07.439254   13284 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0521 08:04:07.425297   50195 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:07.428709   50195 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:07.429555   50195 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:07.431729   50195 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:07.433971   50195 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0521 08:04:07.425297   50195 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:07.428709   50195 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:07.429555   50195 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:07.431729   50195 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:07.433971   50195 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0521 13:34:07.439254   13284 logs.go:123] Gathering logs for kube-apiserver [c1e0c79fbdfc] ...
I0521 13:34:07.439254   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c1e0c79fbdfc"
I0521 13:34:07.505180   13284 logs.go:123] Gathering logs for coredns [2dcca18ff085] ...
I0521 13:34:07.505180   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2dcca18ff085"
I0521 13:34:07.568201   13284 logs.go:123] Gathering logs for kube-controller-manager [c8f2c632c356] ...
I0521 13:34:07.568201   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c8f2c632c356"
I0521 13:34:07.631497   13284 logs.go:123] Gathering logs for dmesg ...
I0521 13:34:07.631497   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0521 13:34:07.660153   13284 logs.go:123] Gathering logs for container status ...
I0521 13:34:07.660153   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0521 13:34:07.698414   13284 logs.go:123] Gathering logs for kubelet ...
I0521 13:34:07.698414   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0521 13:34:07.744498   13284 logs.go:123] Gathering logs for etcd [741730d728ce] ...
I0521 13:34:07.744498   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 741730d728ce"
I0521 13:34:07.833201   13284 logs.go:123] Gathering logs for kube-scheduler [f88690a5517b] ...
I0521 13:34:07.833201   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f88690a5517b"
I0521 13:34:07.909037   13284 logs.go:123] Gathering logs for kube-proxy [e1858dc15ac9] ...
I0521 13:34:07.909037   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1858dc15ac9"
I0521 13:34:10.487981   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:34:10.507285   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0521 13:34:10.528951   13284 logs.go:276] 1 containers: [c1e0c79fbdfc]
I0521 13:34:10.536759   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0521 13:34:10.561312   13284 logs.go:276] 1 containers: [741730d728ce]
I0521 13:34:10.567719   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0521 13:34:10.587613   13284 logs.go:276] 2 containers: [2dcca18ff085 95f2456f0b22]
I0521 13:34:10.595667   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0521 13:34:10.617394   13284 logs.go:276] 1 containers: [f88690a5517b]
I0521 13:34:10.624353   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0521 13:34:10.644443   13284 logs.go:276] 1 containers: [e1858dc15ac9]
I0521 13:34:10.651196   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0521 13:34:10.671635   13284 logs.go:276] 1 containers: [c8f2c632c356]
I0521 13:34:10.679260   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0521 13:34:10.699105   13284 logs.go:276] 0 containers: []
W0521 13:34:10.699105   13284 logs.go:278] No container was found matching "kindnet"
I0521 13:34:10.705936   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0521 13:34:10.727851   13284 logs.go:276] 2 containers: [b279ab02cf1a bd82d4f75a25]
I0521 13:34:10.727851   13284 logs.go:123] Gathering logs for kubelet ...
I0521 13:34:10.727851   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0521 13:34:10.784593   13284 logs.go:123] Gathering logs for kube-scheduler [f88690a5517b] ...
I0521 13:34:10.784593   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f88690a5517b"
I0521 13:34:10.856117   13284 logs.go:123] Gathering logs for kube-proxy [e1858dc15ac9] ...
I0521 13:34:10.856117   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1858dc15ac9"
I0521 13:34:10.913217   13284 logs.go:123] Gathering logs for container status ...
I0521 13:34:10.913217   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0521 13:34:10.956384   13284 logs.go:123] Gathering logs for kube-apiserver [c1e0c79fbdfc] ...
I0521 13:34:10.956384   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c1e0c79fbdfc"
I0521 13:34:11.020911   13284 logs.go:123] Gathering logs for coredns [2dcca18ff085] ...
I0521 13:34:11.020911   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2dcca18ff085"
I0521 13:34:11.098488   13284 logs.go:123] Gathering logs for coredns [95f2456f0b22] ...
I0521 13:34:11.098488   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 95f2456f0b22"
I0521 13:34:11.176140   13284 logs.go:123] Gathering logs for describe nodes ...
I0521 13:34:11.176140   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0521 13:34:11.253735   13284 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0521 08:04:11.242738   50473 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:11.243030   50473 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:11.245216   50473 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:11.246131   50473 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:11.247550   50473 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0521 08:04:11.242738   50473 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:11.243030   50473 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:11.245216   50473 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:11.246131   50473 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:11.247550   50473 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0521 13:34:11.253735   13284 logs.go:123] Gathering logs for etcd [741730d728ce] ...
I0521 13:34:11.253735   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 741730d728ce"
I0521 13:34:11.352298   13284 logs.go:123] Gathering logs for storage-provisioner [b279ab02cf1a] ...
I0521 13:34:11.352298   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b279ab02cf1a"
I0521 13:34:11.408806   13284 logs.go:123] Gathering logs for storage-provisioner [bd82d4f75a25] ...
I0521 13:34:11.408806   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bd82d4f75a25"
I0521 13:34:11.433405   13284 logs.go:123] Gathering logs for dmesg ...
I0521 13:34:11.433405   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0521 13:34:11.450589   13284 logs.go:123] Gathering logs for kube-controller-manager [c8f2c632c356] ...
I0521 13:34:11.450589   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c8f2c632c356"
I0521 13:34:11.520587   13284 logs.go:123] Gathering logs for Docker ...
I0521 13:34:11.520587   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0521 13:34:14.127956   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:34:14.149233   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0521 13:34:14.168669   13284 logs.go:276] 1 containers: [c1e0c79fbdfc]
I0521 13:34:14.176227   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0521 13:34:14.208793   13284 logs.go:276] 1 containers: [741730d728ce]
I0521 13:34:14.215455   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0521 13:34:14.240335   13284 logs.go:276] 2 containers: [2dcca18ff085 95f2456f0b22]
I0521 13:34:14.246318   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0521 13:34:14.264658   13284 logs.go:276] 1 containers: [f88690a5517b]
I0521 13:34:14.272918   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0521 13:34:14.290954   13284 logs.go:276] 1 containers: [e1858dc15ac9]
I0521 13:34:14.297455   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0521 13:34:14.314765   13284 logs.go:276] 1 containers: [c8f2c632c356]
I0521 13:34:14.321991   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0521 13:34:14.339371   13284 logs.go:276] 0 containers: []
W0521 13:34:14.339371   13284 logs.go:278] No container was found matching "kindnet"
I0521 13:34:14.345608   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0521 13:34:14.366481   13284 logs.go:276] 2 containers: [b279ab02cf1a bd82d4f75a25]
I0521 13:34:14.366537   13284 logs.go:123] Gathering logs for coredns [2dcca18ff085] ...
I0521 13:34:14.366537   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2dcca18ff085"
I0521 13:34:14.438247   13284 logs.go:123] Gathering logs for kube-apiserver [c1e0c79fbdfc] ...
I0521 13:34:14.438247   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c1e0c79fbdfc"
I0521 13:34:14.506357   13284 logs.go:123] Gathering logs for storage-provisioner [bd82d4f75a25] ...
I0521 13:34:14.506357   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bd82d4f75a25"
I0521 13:34:14.532769   13284 logs.go:123] Gathering logs for container status ...
I0521 13:34:14.532769   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0521 13:34:14.571070   13284 logs.go:123] Gathering logs for coredns [95f2456f0b22] ...
I0521 13:34:14.571070   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 95f2456f0b22"
I0521 13:34:14.631093   13284 logs.go:123] Gathering logs for dmesg ...
I0521 13:34:14.631093   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0521 13:34:14.659466   13284 logs.go:123] Gathering logs for etcd [741730d728ce] ...
I0521 13:34:14.659466   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 741730d728ce"
I0521 13:34:14.745273   13284 logs.go:123] Gathering logs for kube-scheduler [f88690a5517b] ...
I0521 13:34:14.745273   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f88690a5517b"
I0521 13:34:14.809143   13284 logs.go:123] Gathering logs for kube-proxy [e1858dc15ac9] ...
I0521 13:34:14.809143   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1858dc15ac9"
I0521 13:34:14.875374   13284 logs.go:123] Gathering logs for kube-controller-manager [c8f2c632c356] ...
I0521 13:34:14.875374   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c8f2c632c356"
I0521 13:34:14.936825   13284 logs.go:123] Gathering logs for Docker ...
I0521 13:34:14.936825   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0521 13:34:15.012763   13284 logs.go:123] Gathering logs for kubelet ...
I0521 13:34:15.012763   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0521 13:34:15.071447   13284 logs.go:123] Gathering logs for storage-provisioner [b279ab02cf1a] ...
I0521 13:34:15.071447   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b279ab02cf1a"
I0521 13:34:15.113895   13284 logs.go:123] Gathering logs for describe nodes ...
I0521 13:34:15.113895   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0521 13:34:15.172385   13284 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0521 08:04:15.163956   50774 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:15.164630   50774 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:15.166509   50774 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:15.167072   50774 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:15.168867   50774 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0521 08:04:15.163956   50774 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:15.164630   50774 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:15.166509   50774 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:15.167072   50774 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:15.168867   50774 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0521 13:34:17.687666   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:34:17.727615   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0521 13:34:17.765210   13284 logs.go:276] 1 containers: [c1e0c79fbdfc]
I0521 13:34:17.772340   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0521 13:34:17.798771   13284 logs.go:276] 1 containers: [741730d728ce]
I0521 13:34:17.806043   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0521 13:34:17.829905   13284 logs.go:276] 2 containers: [2dcca18ff085 95f2456f0b22]
I0521 13:34:17.837214   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0521 13:34:17.864477   13284 logs.go:276] 1 containers: [f88690a5517b]
I0521 13:34:17.871681   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0521 13:34:17.898137   13284 logs.go:276] 1 containers: [e1858dc15ac9]
I0521 13:34:17.906976   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0521 13:34:17.933711   13284 logs.go:276] 1 containers: [c8f2c632c356]
I0521 13:34:17.940714   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0521 13:34:17.985474   13284 logs.go:276] 0 containers: []
W0521 13:34:17.985474   13284 logs.go:278] No container was found matching "kindnet"
I0521 13:34:17.992118   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0521 13:34:18.022553   13284 logs.go:276] 2 containers: [b279ab02cf1a bd82d4f75a25]
I0521 13:34:18.022553   13284 logs.go:123] Gathering logs for etcd [741730d728ce] ...
I0521 13:34:18.022553   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 741730d728ce"
I0521 13:34:18.127911   13284 logs.go:123] Gathering logs for storage-provisioner [b279ab02cf1a] ...
I0521 13:34:18.127911   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b279ab02cf1a"
I0521 13:34:18.178990   13284 logs.go:123] Gathering logs for storage-provisioner [bd82d4f75a25] ...
I0521 13:34:18.178990   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bd82d4f75a25"
I0521 13:34:18.198289   13284 logs.go:123] Gathering logs for kube-apiserver [c1e0c79fbdfc] ...
I0521 13:34:18.198289   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c1e0c79fbdfc"
I0521 13:34:18.267732   13284 logs.go:123] Gathering logs for container status ...
I0521 13:34:18.267732   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0521 13:34:18.316749   13284 logs.go:123] Gathering logs for describe nodes ...
I0521 13:34:18.316856   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0521 13:34:18.397684   13284 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0521 08:04:18.386643   50960 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:18.388672   50960 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:18.390066   50960 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:18.390869   50960 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:18.392775   50960 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0521 08:04:18.386643   50960 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:18.388672   50960 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:18.390066   50960 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:18.390869   50960 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:18.392775   50960 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0521 13:34:18.397684   13284 logs.go:123] Gathering logs for kube-controller-manager [c8f2c632c356] ...
I0521 13:34:18.397684   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c8f2c632c356"
I0521 13:34:18.481512   13284 logs.go:123] Gathering logs for Docker ...
I0521 13:34:18.481512   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0521 13:34:18.558788   13284 logs.go:123] Gathering logs for kubelet ...
I0521 13:34:18.558788   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0521 13:34:18.618490   13284 logs.go:123] Gathering logs for dmesg ...
I0521 13:34:18.618490   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0521 13:34:18.634461   13284 logs.go:123] Gathering logs for coredns [2dcca18ff085] ...
I0521 13:34:18.634461   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2dcca18ff085"
I0521 13:34:18.711672   13284 logs.go:123] Gathering logs for coredns [95f2456f0b22] ...
I0521 13:34:18.711672   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 95f2456f0b22"
I0521 13:34:18.836850   13284 logs.go:123] Gathering logs for kube-scheduler [f88690a5517b] ...
I0521 13:34:18.836850   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f88690a5517b"
I0521 13:34:18.927003   13284 logs.go:123] Gathering logs for kube-proxy [e1858dc15ac9] ...
I0521 13:34:18.927003   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1858dc15ac9"
I0521 13:34:21.515636   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:34:21.544832   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0521 13:34:21.566998   13284 logs.go:276] 1 containers: [c1e0c79fbdfc]
I0521 13:34:21.574656   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0521 13:34:21.594781   13284 logs.go:276] 1 containers: [741730d728ce]
I0521 13:34:21.600998   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0521 13:34:21.619772   13284 logs.go:276] 2 containers: [2dcca18ff085 95f2456f0b22]
I0521 13:34:21.627012   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0521 13:34:21.644519   13284 logs.go:276] 1 containers: [f88690a5517b]
I0521 13:34:21.652707   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0521 13:34:21.670099   13284 logs.go:276] 1 containers: [e1858dc15ac9]
I0521 13:34:21.677494   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0521 13:34:21.697249   13284 logs.go:276] 1 containers: [c8f2c632c356]
I0521 13:34:21.704722   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0521 13:34:21.723353   13284 logs.go:276] 0 containers: []
W0521 13:34:21.723353   13284 logs.go:278] No container was found matching "kindnet"
I0521 13:34:21.730940   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0521 13:34:21.755723   13284 logs.go:276] 2 containers: [b279ab02cf1a bd82d4f75a25]
I0521 13:34:21.755723   13284 logs.go:123] Gathering logs for dmesg ...
I0521 13:34:21.755723   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0521 13:34:21.773308   13284 logs.go:123] Gathering logs for coredns [95f2456f0b22] ...
I0521 13:34:21.773308   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 95f2456f0b22"
I0521 13:34:21.849640   13284 logs.go:123] Gathering logs for storage-provisioner [bd82d4f75a25] ...
I0521 13:34:21.849640   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bd82d4f75a25"
I0521 13:34:21.877458   13284 logs.go:123] Gathering logs for Docker ...
I0521 13:34:21.877458   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0521 13:34:21.953048   13284 logs.go:123] Gathering logs for container status ...
I0521 13:34:21.953048   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0521 13:34:22.000772   13284 logs.go:123] Gathering logs for kube-apiserver [c1e0c79fbdfc] ...
I0521 13:34:22.000772   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c1e0c79fbdfc"
I0521 13:34:22.079832   13284 logs.go:123] Gathering logs for etcd [741730d728ce] ...
I0521 13:34:22.079832   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 741730d728ce"
I0521 13:34:22.186017   13284 logs.go:123] Gathering logs for kube-scheduler [f88690a5517b] ...
I0521 13:34:22.186017   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f88690a5517b"
I0521 13:34:22.266698   13284 logs.go:123] Gathering logs for kube-proxy [e1858dc15ac9] ...
I0521 13:34:22.266698   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1858dc15ac9"
I0521 13:34:22.328413   13284 logs.go:123] Gathering logs for storage-provisioner [b279ab02cf1a] ...
I0521 13:34:22.328413   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b279ab02cf1a"
I0521 13:34:22.379889   13284 logs.go:123] Gathering logs for kubelet ...
I0521 13:34:22.379889   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0521 13:34:22.433980   13284 logs.go:123] Gathering logs for describe nodes ...
I0521 13:34:22.433980   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0521 13:34:22.500787   13284 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0521 08:04:22.492291   51240 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:22.492765   51240 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:22.494447   51240 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:22.494719   51240 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:22.496489   51240 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0521 08:04:22.492291   51240 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:22.492765   51240 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:22.494447   51240 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:22.494719   51240 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:22.496489   51240 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0521 13:34:22.500787   13284 logs.go:123] Gathering logs for coredns [2dcca18ff085] ...
I0521 13:34:22.500787   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2dcca18ff085"
I0521 13:34:22.583838   13284 logs.go:123] Gathering logs for kube-controller-manager [c8f2c632c356] ...
I0521 13:34:22.583838   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c8f2c632c356"
I0521 13:34:24.715455   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0521 13:34:24.780793   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0521 13:34:24.780793   13284 out.go:239] ❗  Enabling 'storage-provisioner' returned an error: running callbacks: [sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
]
I0521 13:34:25.179498   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:34:25.201956   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0521 13:34:25.224813   13284 logs.go:276] 1 containers: [c1e0c79fbdfc]
I0521 13:34:25.227016   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0521 13:34:25.244296   13284 logs.go:276] 1 containers: [741730d728ce]
I0521 13:34:25.246433   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0521 13:34:25.263976   13284 logs.go:276] 2 containers: [2dcca18ff085 95f2456f0b22]
I0521 13:34:25.266139   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0521 13:34:25.285283   13284 logs.go:276] 1 containers: [f88690a5517b]
I0521 13:34:25.287918   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0521 13:34:25.308710   13284 logs.go:276] 1 containers: [e1858dc15ac9]
I0521 13:34:25.310762   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0521 13:34:25.330182   13284 logs.go:276] 1 containers: [c8f2c632c356]
I0521 13:34:25.332403   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0521 13:34:25.354336   13284 logs.go:276] 0 containers: []
W0521 13:34:25.354336   13284 logs.go:278] No container was found matching "kindnet"
I0521 13:34:25.356457   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0521 13:34:25.377147   13284 logs.go:276] 2 containers: [b279ab02cf1a bd82d4f75a25]
I0521 13:34:25.377147   13284 logs.go:123] Gathering logs for etcd [741730d728ce] ...
I0521 13:34:25.377147   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 741730d728ce"
I0521 13:34:25.435869   13284 logs.go:123] Gathering logs for storage-provisioner [b279ab02cf1a] ...
I0521 13:34:25.435869   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b279ab02cf1a"
I0521 13:34:25.470471   13284 logs.go:123] Gathering logs for Docker ...
I0521 13:34:25.470471   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0521 13:34:25.515886   13284 logs.go:123] Gathering logs for kube-apiserver [c1e0c79fbdfc] ...
I0521 13:34:25.515886   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c1e0c79fbdfc"
I0521 13:34:25.565453   13284 logs.go:123] Gathering logs for dmesg ...
I0521 13:34:25.565453   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0521 13:34:25.581858   13284 logs.go:123] Gathering logs for describe nodes ...
I0521 13:34:25.581858   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0521 13:34:25.654171   13284 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0521 08:04:25.644275   51428 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:25.645045   51428 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:25.647098   51428 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:25.647794   51428 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:25.649882   51428 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0521 08:04:25.644275   51428 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:25.645045   51428 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:25.647098   51428 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:25.647794   51428 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:25.649882   51428 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0521 13:34:25.654171   13284 logs.go:123] Gathering logs for coredns [2dcca18ff085] ...
I0521 13:34:25.654171   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2dcca18ff085"
I0521 13:34:25.751047   13284 logs.go:123] Gathering logs for coredns [95f2456f0b22] ...
I0521 13:34:25.751047   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 95f2456f0b22"
I0521 13:34:25.841516   13284 logs.go:123] Gathering logs for storage-provisioner [bd82d4f75a25] ...
I0521 13:34:25.841516   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bd82d4f75a25"
I0521 13:34:25.865345   13284 logs.go:123] Gathering logs for container status ...
I0521 13:34:25.865345   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0521 13:34:25.910076   13284 logs.go:123] Gathering logs for kubelet ...
I0521 13:34:25.910076   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0521 13:34:25.944264   13284 logs.go:123] Gathering logs for kube-controller-manager [c8f2c632c356] ...
I0521 13:34:25.944264   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c8f2c632c356"
I0521 13:34:25.992763   13284 logs.go:123] Gathering logs for kube-proxy [e1858dc15ac9] ...
I0521 13:34:25.992763   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1858dc15ac9"
I0521 13:34:26.030878   13284 logs.go:123] Gathering logs for kube-scheduler [f88690a5517b] ...
I0521 13:34:26.030878   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f88690a5517b"
I0521 13:34:28.592897   13284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0521 13:34:28.606130   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0521 13:34:28.623461   13284 logs.go:276] 1 containers: [c1e0c79fbdfc]
I0521 13:34:28.625647   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0521 13:34:28.643184   13284 logs.go:276] 1 containers: [741730d728ce]
I0521 13:34:28.648020   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0521 13:34:28.664440   13284 logs.go:276] 2 containers: [2dcca18ff085 95f2456f0b22]
I0521 13:34:28.666029   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0521 13:34:28.684517   13284 logs.go:276] 1 containers: [f88690a5517b]
I0521 13:34:28.686646   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0521 13:34:28.704543   13284 logs.go:276] 1 containers: [e1858dc15ac9]
I0521 13:34:28.706697   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0521 13:34:28.723573   13284 logs.go:276] 1 containers: [c8f2c632c356]
I0521 13:34:28.725713   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0521 13:34:28.739484   13284 logs.go:276] 0 containers: []
W0521 13:34:28.739484   13284 logs.go:278] No container was found matching "kindnet"
I0521 13:34:28.748237   13284 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0521 13:34:28.767434   13284 logs.go:276] 2 containers: [b279ab02cf1a bd82d4f75a25]
I0521 13:34:28.767434   13284 logs.go:123] Gathering logs for coredns [2dcca18ff085] ...
I0521 13:34:28.767434   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2dcca18ff085"
I0521 13:34:28.813558   13284 logs.go:123] Gathering logs for kube-scheduler [f88690a5517b] ...
I0521 13:34:28.813558   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f88690a5517b"
I0521 13:34:28.864299   13284 logs.go:123] Gathering logs for kubelet ...
I0521 13:34:28.864299   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0521 13:34:28.873782   13284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0521 13:34:28.898473   13284 logs.go:123] Gathering logs for dmesg ...
I0521 13:34:28.898473   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
W0521 13:34:28.959822   13284 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0521 13:34:28.959822   13284 out.go:239] ❗  Enabling 'default-storageclass' returned an error: running callbacks: [sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
]
I0521 13:34:28.963221   13284 out.go:177] 🌟  Enabled addons: 
I0521 13:34:28.962027   13284 logs.go:123] Gathering logs for storage-provisioner [bd82d4f75a25] ...
I0521 13:34:28.963221   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bd82d4f75a25"
I0521 13:34:28.964320   13284 addons.go:505] duration metric: took 1m44.2788241s for enable addons: enabled=[]
I0521 13:34:29.006882   13284 logs.go:123] Gathering logs for Docker ...
I0521 13:34:29.006882   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0521 13:34:29.073708   13284 logs.go:123] Gathering logs for container status ...
I0521 13:34:29.073708   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0521 13:34:29.125265   13284 logs.go:123] Gathering logs for coredns [95f2456f0b22] ...
I0521 13:34:29.125265   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 95f2456f0b22"
I0521 13:34:29.179977   13284 logs.go:123] Gathering logs for kube-proxy [e1858dc15ac9] ...
I0521 13:34:29.179977   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1858dc15ac9"
I0521 13:34:29.233673   13284 logs.go:123] Gathering logs for kube-apiserver [c1e0c79fbdfc] ...
I0521 13:34:29.233673   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c1e0c79fbdfc"
I0521 13:34:29.281132   13284 logs.go:123] Gathering logs for kube-controller-manager [c8f2c632c356] ...
I0521 13:34:29.281132   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c8f2c632c356"
I0521 13:34:29.327945   13284 logs.go:123] Gathering logs for storage-provisioner [b279ab02cf1a] ...
I0521 13:34:29.327945   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b279ab02cf1a"
I0521 13:34:29.365504   13284 logs.go:123] Gathering logs for describe nodes ...
I0521 13:34:29.365504   13284 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0521 13:34:29.433334   13284 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0521 08:04:29.425457   51743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:29.426312   51743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:29.427453   51743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:29.428048   51743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:29.429907   51743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0521 08:04:29.425457   51743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:29.426312   51743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:29.427453   51743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:29.428048   51743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:29.429907   51743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0521 13:34:29.433334   13284 logs.go:123] Gathering logs for etcd [741730d728ce] ...
I0521 13:34:29.433334   13284 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 741730d728ce"


==> Docker <==
May 21 08:04:07 minikube dockerd[46338]: 2024/05/21 08:04:07 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:07 minikube dockerd[46338]: 2024/05/21 08:04:07 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:07 minikube dockerd[46338]: 2024/05/21 08:04:07 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:07 minikube dockerd[46338]: 2024/05/21 08:04:07 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:07 minikube dockerd[46338]: 2024/05/21 08:04:07 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:07 minikube dockerd[46338]: 2024/05/21 08:04:07 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:10 minikube dockerd[46338]: 2024/05/21 08:04:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:10 minikube dockerd[46338]: 2024/05/21 08:04:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:10 minikube dockerd[46338]: 2024/05/21 08:04:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:11 minikube dockerd[46338]: 2024/05/21 08:04:11 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:11 minikube dockerd[46338]: 2024/05/21 08:04:11 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:11 minikube dockerd[46338]: 2024/05/21 08:04:11 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:11 minikube dockerd[46338]: 2024/05/21 08:04:11 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:11 minikube dockerd[46338]: 2024/05/21 08:04:11 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:11 minikube dockerd[46338]: 2024/05/21 08:04:11 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:14 minikube dockerd[46338]: 2024/05/21 08:04:14 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:14 minikube dockerd[46338]: 2024/05/21 08:04:14 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:14 minikube dockerd[46338]: 2024/05/21 08:04:14 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:14 minikube dockerd[46338]: 2024/05/21 08:04:14 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:14 minikube dockerd[46338]: 2024/05/21 08:04:14 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:14 minikube dockerd[46338]: 2024/05/21 08:04:14 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:14 minikube dockerd[46338]: 2024/05/21 08:04:14 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:14 minikube dockerd[46338]: 2024/05/21 08:04:14 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:15 minikube dockerd[46338]: 2024/05/21 08:04:15 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:18 minikube dockerd[46338]: 2024/05/21 08:04:18 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:18 minikube dockerd[46338]: 2024/05/21 08:04:18 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:18 minikube dockerd[46338]: 2024/05/21 08:04:18 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:18 minikube dockerd[46338]: 2024/05/21 08:04:18 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:18 minikube dockerd[46338]: 2024/05/21 08:04:18 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:18 minikube dockerd[46338]: 2024/05/21 08:04:18 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:18 minikube dockerd[46338]: 2024/05/21 08:04:18 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:18 minikube dockerd[46338]: 2024/05/21 08:04:18 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:18 minikube dockerd[46338]: 2024/05/21 08:04:18 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:21 minikube dockerd[46338]: 2024/05/21 08:04:21 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:21 minikube dockerd[46338]: 2024/05/21 08:04:21 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:22 minikube dockerd[46338]: 2024/05/21 08:04:22 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:22 minikube dockerd[46338]: 2024/05/21 08:04:22 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:22 minikube dockerd[46338]: 2024/05/21 08:04:22 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:22 minikube dockerd[46338]: 2024/05/21 08:04:22 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:22 minikube dockerd[46338]: 2024/05/21 08:04:22 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:22 minikube dockerd[46338]: 2024/05/21 08:04:22 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:22 minikube dockerd[46338]: 2024/05/21 08:04:22 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:25 minikube dockerd[46338]: 2024/05/21 08:04:25 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:25 minikube dockerd[46338]: 2024/05/21 08:04:25 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:25 minikube dockerd[46338]: 2024/05/21 08:04:25 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:25 minikube dockerd[46338]: 2024/05/21 08:04:25 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:25 minikube dockerd[46338]: 2024/05/21 08:04:25 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:25 minikube dockerd[46338]: 2024/05/21 08:04:25 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:25 minikube dockerd[46338]: 2024/05/21 08:04:25 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:26 minikube dockerd[46338]: 2024/05/21 08:04:26 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:26 minikube dockerd[46338]: 2024/05/21 08:04:26 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:28 minikube dockerd[46338]: 2024/05/21 08:04:28 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:28 minikube dockerd[46338]: 2024/05/21 08:04:28 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:29 minikube dockerd[46338]: 2024/05/21 08:04:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:29 minikube dockerd[46338]: 2024/05/21 08:04:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:29 minikube dockerd[46338]: 2024/05/21 08:04:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:29 minikube dockerd[46338]: 2024/05/21 08:04:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:29 minikube dockerd[46338]: 2024/05/21 08:04:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:29 minikube dockerd[46338]: 2024/05/21 08:04:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
May 21 08:04:29 minikube dockerd[46338]: 2024/05/21 08:04:29 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
eff476b41586f       d53bf3ada6ed9       40 hours ago        Exited              myfirstpod                0                   4e327b70ea4c4       myfirstpodrs-6trxx
a96886a2f2f17       d53bf3ada6ed9       40 hours ago        Exited              myfirstpod                0                   fe2b1457981bd       myfirstpodrc-lvnjd
748a1f7a911e3       d53bf3ada6ed9       40 hours ago        Exited              myfirstpod                0                   0fd87f365315e       myfirstpodrc-68x9w
004dd2e3de058       d53bf3ada6ed9       40 hours ago        Exited              myfirstpod                0                   311904ec29b6b       myfirstpod
87ceb925e792d       d53bf3ada6ed9       40 hours ago        Exited              myfirstpod                0                   8841679c99661       myfirstpodrs-2cfpd
3bf5863425138       d53bf3ada6ed9       40 hours ago        Exited              myfirstpod                0                   38bd1d8600bb5       myfirstpodrs-xvxnd
2807961a494d7       d53bf3ada6ed9       40 hours ago        Exited              myfirstpod                0                   4162dea218eff       myfirstpodrs-w9v5c
b279ab02cf1a2       6e38f40d628db       40 hours ago        Exited              storage-provisioner       1                   bfc1d7dc6fc5d       storage-provisioner
95f2456f0b22f       cbb01a7bd410d       40 hours ago        Exited              coredns                   0                   b202071eeb4aa       coredns-7db6d8ff4d-8qjst
2dcca18ff085c       cbb01a7bd410d       40 hours ago        Exited              coredns                   0                   1cf7a7cd0f35e       coredns-7db6d8ff4d-wqbf5
e1858dc15ac99       a0bf559e280cf       40 hours ago        Exited              kube-proxy                0                   eba3fc2855dcf       kube-proxy-fwgnj
bd82d4f75a257       6e38f40d628db       40 hours ago        Exited              storage-provisioner       0                   bfc1d7dc6fc5d       storage-provisioner
c1e0c79fbdfc2       c42f13656d0b2       40 hours ago        Exited              kube-apiserver            0                   a8e45f319d824       kube-apiserver-minikube
c8f2c632c3568       c7aad43836fa5       40 hours ago        Exited              kube-controller-manager   0                   1c6c7c882cac3       kube-controller-manager-minikube
741730d728ce4       3861cfcd7c04c       40 hours ago        Exited              etcd                      0                   6ee467d00d196       etcd-minikube
f88690a5517b0       259c8277fcbbc       40 hours ago        Exited              kube-scheduler            0                   4ecd3017f9d84       kube-scheduler-minikube


==> coredns [2dcca18ff085] <==
Trace[578468800]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=1533": net/http: TLS handshake timeout 39696ms (17:18:25.939)
Trace[578468800]: [43.091122938s] [43.091122938s] END
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=1533": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: Trace[52446664]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (19-May-2024 17:17:54.740) (total time: 78687ms):
Trace[52446664]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=1533": net/http: TLS handshake timeout 71487ms (17:19:06.224)
Trace[52446664]: [1m18.687437096s] [1m18.687437096s] END
[INFO] plugin/kubernetes: Trace[926610577]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (19-May-2024 17:18:16.229) (total time: 58716ms):
Trace[926610577]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?resourceVersion=1533": net/http: TLS handshake timeout 55897ms (17:19:12.122)
Trace[926610577]: [58.716514563s] [58.716514563s] END
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=1533": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: Trace[1241668472]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (19-May-2024 17:18:48.948) (total time: 72482ms):
Trace[1241668472]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=1533": net/http: TLS handshake timeout 62976ms (17:19:51.918)
Trace[1241668472]: [1m12.482863006s] [1m12.482863006s] END
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=1533": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[665740679]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (19-May-2024 17:19:29.821) (total time: 68404ms):
Trace[665740679]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?resourceVersion=1533": net/http: TLS handshake timeout 63102ms (17:20:32.919)
Trace[665740679]: [1m8.404556309s] [1m8.404556309s] END
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: Trace[340724448]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (19-May-2024 17:19:28.136) (total time: 83891ms):
Trace[340724448]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=1533": net/http: TLS handshake timeout 70489ms (17:20:38.621)
Trace[340724448]: [1m23.89111865s] [1m23.89111865s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: Trace[789265948]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (19-May-2024 17:20:34.036) (total time: 104894ms):
Trace[789265948]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=1533": net/http: TLS handshake timeout 95395ms (17:22:09.426)
Trace[789265948]: [1m44.894001022s] [1m44.894001022s] END
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)


==> coredns [95f2456f0b22] <==
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: Trace[1726340998]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (19-May-2024 17:18:16.226) (total time: 59121ms):
Trace[1726340998]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?resourceVersion=1533": net/http: TLS handshake timeout 54805ms (17:19:11.028)
Trace[1726340998]: [59.121656382s] [59.121656382s] END
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=1533": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: Trace[1951661642]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (19-May-2024 17:18:16.935) (total time: 72796ms):
Trace[1951661642]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=1533": net/http: TLS handshake timeout 63294ms (17:19:20.225)
Trace[1951661642]: [1m12.796264121s] [1m12.796264121s] END
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: Waited for 1.19576866s due to client-side throttling, not priority and fairness, request: GET:https://10.96.0.1:443/api/v1/services?resourceVersion=1533
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: Trace[421445161]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (19-May-2024 17:18:51.028) (total time: 74610ms):
Trace[421445161]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=1533": net/http: TLS handshake timeout 69404ms (17:20:00.428)
Trace[421445161]: [1m14.610971468s] [1m14.610971468s] END
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=1533": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Waited for 1.093689906s due to client-side throttling, not priority and fairness, request: GET:https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=1533
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: Trace[1517633421]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (19-May-2024 17:19:27.824) (total time: 96598ms):
Trace[1517633421]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?resourceVersion=1533": net/http: TLS handshake timeout 91104ms (17:20:58.921)
Trace[1517633421]: [1m36.598381309s] [1m36.598381309s] END
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: Trace[1921952391]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (19-May-2024 17:20:00.429) (total time: 121694ms):
Trace[1921952391]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=1533": net/http: TLS handshake timeout 110896ms (17:21:51.317)
Trace[1921952391]: [2m1.694716823s] [2m1.694716823s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: Trace[1439762524]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (19-May-2024 17:20:38.827) (total time: 117992ms):
Trace[1439762524]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=1533": net/http: TLS handshake timeout 98492ms (17:22:17.314)
Trace[1439762524]: [1m57.992865807s] [1m57.992865807s] END
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=1533": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)


==> describe nodes <==
command /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" failed with error: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0521 08:04:34.935313   51968 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:34.936352   51968 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:34.938407   51968 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:34.939029   51968 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
E0521 08:04:34.940742   51968 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp [::1]:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?


==> dmesg <==
[  +0.000737] FS-Cache: O-cookie c=00000008 [p=00000002 fl=222 nc=0 na=1]
[  +0.000740] FS-Cache: O-cookie d=00000000a4d79885{9P.session} n=00000000b9b56faa
[  +0.000693] FS-Cache: O-key=[10] '34323934393337353636'
[  +0.000433] FS-Cache: N-cookie c=00000009 [p=00000002 fl=2 nc=0 na=1]
[  +0.000522] FS-Cache: N-cookie d=00000000a4d79885{9P.session} n=00000000824dc0ab
[  +0.000741] FS-Cache: N-key=[10] '34323934393337353636'
[  +0.048157] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000009]  failed 2
[  +0.008165] FS-Cache: Duplicate cookie detected
[  +0.000654] FS-Cache: O-cookie c=0000000b [p=00000002 fl=222 nc=0 na=1]
[  +0.000729] FS-Cache: O-cookie d=00000000a4d79885{9P.session} n=00000000216b4992
[  +0.000916] FS-Cache: O-key=[10] '34323934393337353732'
[  +0.000600] FS-Cache: N-cookie c=0000000c [p=00000002 fl=2 nc=0 na=1]
[  +0.001065] FS-Cache: N-cookie d=00000000a4d79885{9P.session} n=000000004a5c559d
[  +0.001415] FS-Cache: N-key=[10] '34323934393337353732'
[  +0.018685] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.296153] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001106] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000848] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001038] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.626018] FS-Cache: Duplicate cookie detected
[  +0.000762] FS-Cache: O-cookie c=00000017 [p=00000002 fl=222 nc=0 na=1]
[  +0.000769] FS-Cache: O-cookie d=00000000a4d79885{9P.session} n=00000000c8be4778
[  +0.000876] FS-Cache: O-key=[10] '34323934393337373637'
[  +0.000638] FS-Cache: N-cookie c=00000018 [p=00000002 fl=2 nc=0 na=1]
[  +0.000717] FS-Cache: N-cookie d=00000000a4d79885{9P.session} n=000000008dff353d
[  +0.000886] FS-Cache: N-key=[10] '34323934393337373637'
[  +0.038991] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.004269] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.003602] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.003729] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000009]  failed 2
[  +0.005307] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.002084] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.002636] WSL (4) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001105] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.002422] WSL (5) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001352] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.024225] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.070055] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002074] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002071] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001417] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.464868] netlink: 'init': attribute type 4 has an invalid length.
[May21 07:52] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.004101] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[ +12.342254] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000040] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[ +12.859532] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000017] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +9.446581] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000032] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[May21 07:53] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000018] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +3.803626] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000035] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.


==> etcd [741730d728ce] <==
{"level":"warn","ts":"2024-05-19T17:18:36.041673Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e61a2","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:18:57.134908Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e68da","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:17.726122Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e69c6","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:36.039999Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e5ec2","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:18:48.438607Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e5ec2","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:18:45.026953Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e74c5","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:45.241125Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e74c5","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:18:53.030169Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e69c6","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:19.324199Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e7097","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:05.738198Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e74c5","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:41.834228Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e61a2","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:31.03475Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e74c5","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:30.918091Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e62f7","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:36.021079Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e68da","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:46.438811Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e61a2","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:18:40.338308Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e62f7","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:18:41.532029Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e68da","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:51.018443Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e61a2","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:01.24185Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e69c6","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:18:54.623073Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e6af4","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:52.435711Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e7097","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:18:52.626679Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e68da","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:28.34202Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e67a6","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:51.120125Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e69c6","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:50.138197Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e62f7","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:41.635397Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e7097","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:02.62882Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e68da","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:01.021728Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e69c6","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:18:52.437051Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e68da","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:19.424707Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e7334","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:33.51757Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e61a2","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:50.819001Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e62f7","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:15.818251Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e69c6","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:05.335654Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e6af4","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:56.723413Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e74c5","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:33.119031Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e6af4","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:11.933633Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e68da","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:43.416244Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e67a6","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:16.435256Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e62f7","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:33.019222Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e67a6","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:19:47.032197Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e6af4","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:09.124427Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e7334","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:02.231601Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e7097","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:21.733895Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e62f7","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:55.528802Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e68da","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:54.130644Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e68da","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:52.91383Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e62f7","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:49.934929Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e6af4","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:24.22921Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e67a6","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:51.030934Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e7097","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:33.518732Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e67a6","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:05.433221Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e62f7","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:23.924912Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e6af4","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:48.131583Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e74c5","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:23.924992Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e67a6","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:20:06.8238Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e69c6","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:21:05.018971Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e6af4","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:22:08.22118Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e5ec2","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:22:11.01463Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e5ec2","error":"context deadline exceeded"}
{"level":"warn","ts":"2024-05-19T17:22:24.508396Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc8f917e1e69c6","error":"context deadline exceeded"}


==> kernel <==
 08:04:34 up 42 min,  0 users,  load average: 0.93, 1.50, 1.60
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [c1e0c79fbdfc] <==
Trace[1788445147]: [1.500973013s] [1.500973013s] END
E0519 16:49:54.332804       1 timeout.go:142] post-timeout activity - time-elapsed: 1.397398601s, GET "/api/v1/nodes/minikube" result: <nil>
E0519 16:49:54.333020       1 timeout.go:142] post-timeout activity - time-elapsed: 1.3132051s, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result: <nil>
I0519 16:49:55.626314       1 trace.go:236] Trace[768131557]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:bf39b371-aa42-4e69-94d0-cba8fdfd8a45,client:10.244.0.2,api-group:,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:namespaces,scope:cluster,url:/api/v1/namespaces,user-agent:coredns/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (19-May-2024 16:49:54.326) (total time: 999ms):
Trace[768131557]: ---"Writing http response done" count:4 599ms (16:49:55.326)
Trace[768131557]: [999.677212ms] [999.677212ms] END
I0519 16:49:55.627722       1 trace.go:236] Trace[1006156061]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f8d7b35c-6ecd-4daf-b427-8f45919781b4,client:192.168.49.2,api-group:,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:kube-scheduler/v1.30.0 (linux/amd64) kubernetes/7c48c2b/scheduler,verb:LIST (19-May-2024 16:49:54.633) (total time: 992ms):
Trace[1006156061]: ---"Writing http response done" count:3 798ms (16:49:55.625)
Trace[1006156061]: [992.076768ms] [992.076768ms] END
I0519 16:49:56.437388       1 trace.go:236] Trace[1069282597]: "List" accept:application/json, */*,audit-id:93b7ce67-f1ec-441d-95b4-9d94d0639bc4,client:192.168.49.2,api-group:,api-version:v1,name:extension-apiserver-authentication,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:kube-scheduler/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:GET (19-May-2024 16:49:54.728) (total time: 1501ms):
Trace[1069282597]: ---"Writing http response done" count:1 1207ms (16:49:56.230)
Trace[1069282597]: [1.501997403s] [1.501997403s] END
I0519 16:49:55.628298       1 trace.go:236] Trace[1659074680]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:85b65cf5-b161-4620-a27d-5a9d7d24bb27,client:192.168.49.2,api-group:node.k8s.io,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:runtimeclasses,scope:cluster,url:/apis/node.k8s.io/v1/runtimeclasses,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:LIST (19-May-2024 16:49:54.326) (total time: 1302ms):
Trace[1659074680]: ---"About to List from storage" 100ms (16:49:54.426)
Trace[1659074680]: ---"Writing http response done" count:0 798ms (16:49:55.628)
Trace[1659074680]: [1.302180283s] [1.302180283s] END
I0519 16:49:55.631194       1 trace.go:236] Trace[476300790]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b825ec81-156e-4fd0-b625-f9e457ec86a2,client:192.168.49.2,api-group:policy,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:poddisruptionbudgets,scope:cluster,url:/apis/policy/v1/poddisruptionbudgets,user-agent:kube-scheduler/v1.30.0 (linux/amd64) kubernetes/7c48c2b/scheduler,verb:LIST (19-May-2024 16:49:54.727) (total time: 605ms):
Trace[476300790]: ---"Writing http response done" count:0 399ms (16:49:55.332)
Trace[476300790]: [605.118424ms] [605.118424ms] END
I0519 16:49:55.631240       1 trace.go:236] Trace[805465350]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:bd409cf1-fd5d-4e8d-b015-1d708e816516,client:192.168.49.2,api-group:storage.k8s.io,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:storageclasses,scope:cluster,url:/apis/storage.k8s.io/v1/storageclasses,user-agent:kube-scheduler/v1.30.0 (linux/amd64) kubernetes/7c48c2b/scheduler,verb:LIST (19-May-2024 16:49:54.833) (total time: 500ms):
Trace[805465350]: ---"Writing http response done" count:1 305ms (16:49:55.333)
Trace[805465350]: [500.180189ms] [500.180189ms] END
I0519 16:49:55.631252       1 trace.go:236] Trace[1467751988]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b7207e24-ea43-43f1-8cee-2a156da3417b,client:192.168.49.2,api-group:,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:kube-scheduler/v1.30.0 (linux/amd64) kubernetes/7c48c2b/scheduler,verb:LIST (19-May-2024 16:49:54.326) (total time: 1017ms):
Trace[1467751988]: ---"About to List from storage" 303ms (16:49:54.629)
Trace[1467751988]: ---"Writing http response done" count:15 511ms (16:49:55.343)
Trace[1467751988]: [1.01702983s] [1.01702983s] END
I0519 16:49:55.732139       1 trace.go:236] Trace[787544979]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7a770a26-6268-452b-8b0f-0035676d24b5,client:10.244.0.2,api-group:,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:coredns/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (19-May-2024 16:49:54.332) (total time: 994ms):
Trace[787544979]: ---"Writing http response done" count:3 599ms (16:49:55.326)
Trace[787544979]: [994.223978ms] [994.223978ms] END
I0519 16:49:57.032568       1 trace.go:236] Trace[647162080]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ae43fbb4-c2e0-4ac3-af61-f002b408d7a1,client:192.168.49.2,api-group:,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:persistentvolumes,scope:cluster,url:/api/v1/persistentvolumes,user-agent:kube-scheduler/v1.30.0 (linux/amd64) kubernetes/7c48c2b/scheduler,verb:LIST (19-May-2024 16:49:55.327) (total time: 1695ms):
Trace[647162080]: ---"Writing http response done" count:0 1291ms (16:49:57.023)
Trace[647162080]: [1.695899978s] [1.695899978s] END
I0519 16:49:57.035991       1 trace.go:236] Trace[1850474128]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:bc41a43f-0912-423a-93a1-1225dc04dba7,client:192.168.49.2,api-group:,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:persistentvolumeclaims,scope:cluster,url:/api/v1/persistentvolumeclaims,user-agent:kube-scheduler/v1.30.0 (linux/amd64) kubernetes/7c48c2b/scheduler,verb:LIST (19-May-2024 16:49:56.326) (total time: 605ms):
Trace[1850474128]: ---"Writing http response done" count:0 100ms (16:49:56.932)
Trace[1850474128]: [605.459001ms] [605.459001ms] END
I0519 16:49:57.228692       1 trace.go:236] Trace[1790724089]: "List(recursive=true) etcd3" audit-id:,key:/masterleases/,resourceVersion:0,resourceVersionMatch:NotOlderThan,limit:0,continue: (19-May-2024 16:49:55.328) (total time: 1900ms):
Trace[1790724089]: [1.900566341s] [1.900566341s] END
I0519 16:49:57.235942       1 trace.go:236] Trace[1102672165]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:25a97cfe-59ee-449c-8f05-fc0e28280958,client:192.168.49.2,api-group:apps,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:replicasets,scope:cluster,url:/apis/apps/v1/replicasets,user-agent:kube-scheduler/v1.30.0 (linux/amd64) kubernetes/7c48c2b/scheduler,verb:LIST (19-May-2024 16:49:56.326) (total time: 909ms):
Trace[1102672165]: ---"Writing http response done" count:2 407ms (16:49:57.235)
Trace[1102672165]: [909.587672ms] [909.587672ms] END
I0519 16:49:57.436452       1 trace.go:236] Trace[954686691]: "GuaranteedUpdate etcd3" audit-id:,key:/ranges/serviceips,type:*core.RangeAllocation,resource:serviceipallocations (19-May-2024 16:49:55.625) (total time: 1810ms):
Trace[954686691]: ---"initial value restored" 1801ms (16:49:57.427)
Trace[954686691]: [1.810462684s] [1.810462684s] END
I0519 16:49:57.436652       1 trace.go:236] Trace[617013997]: "GuaranteedUpdate etcd3" audit-id:,key:/ranges/servicenodeports,type:*core.RangeAllocation,resource:servicenodeportallocations (19-May-2024 16:49:55.332) (total time: 2103ms):
Trace[617013997]: ---"initial value restored" 2103ms (16:49:57.435)
Trace[617013997]: [2.103855797s] [2.103855797s] END
E0519 16:49:57.837632       1 timeout.go:142] post-timeout activity - time-elapsed: 4.796329999s, GET "/readyz" result: <nil>
E0519 16:49:57.925064       1 controller.go:159] unable to sync kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0519 16:49:58.327982       1 trace.go:236] Trace[1179252376]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:779c76eb-a6cb-4ab3-a180-1c2227a0dbf8,client:192.168.49.2,api-group:,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:namespaces,scope:cluster,url:/api/v1/namespaces,user-agent:kube-scheduler/v1.30.0 (linux/amd64) kubernetes/7c48c2b/scheduler,verb:LIST (19-May-2024 16:49:57.639) (total time: 684ms):
Trace[1179252376]: ---"Writing http response done" count:4 291ms (16:49:58.324)
Trace[1179252376]: [684.37518ms] [684.37518ms] END
I0519 16:49:58.336907       1 trace.go:236] Trace[1686927837]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:30076344-adc5-4fad-ab37-1a06412e83d5,client:192.168.49.2,api-group:,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:replicationcontrollers,scope:cluster,url:/api/v1/replicationcontrollers,user-agent:kube-scheduler/v1.30.0 (linux/amd64) kubernetes/7c48c2b/scheduler,verb:LIST (19-May-2024 16:49:57.630) (total time: 704ms):
Trace[1686927837]: ---"Writing http response done" count:1 304ms (16:49:58.334)
Trace[1686927837]: [704.489964ms] [704.489964ms] END
I0519 16:49:58.422222       1 trace.go:236] Trace[510112488]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a5b82bd7-3678-43b3-bbaa-5bb0db600e32,client:192.168.49.2,api-group:,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:kube-scheduler/v1.30.0 (linux/amd64) kubernetes/7c48c2b/scheduler,verb:LIST (19-May-2024 16:49:57.529) (total time: 698ms):
Trace[510112488]: ---"Writing http response done" count:1 205ms (16:49:58.227)
Trace[510112488]: [698.358153ms] [698.358153ms] END
E0519 16:53:30.220268       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0519 17:03:59.351695       1 finisher.go:157] FinishRequest: post-timeout activity, waited for 7m18.096016449s, child goroutine has not returned yet
E0519 17:05:05.549138       1 finisher.go:157] FinishRequest: post-timeout activity, waited for 10m15.994087021s, child goroutine has not returned yet


==> kube-controller-manager [c8f2c632c356] <==
Trace[1623287424]: [16m42.01688754s] [16m42.01688754s] END
I0519 17:18:07.141503       1 trace.go:236] Trace[781559729]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 17:00:09.084) (total time: 875616ms):
Trace[781559729]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/admissionregistration.k8s.io/v1/validatingadmissionpolicybindings?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 847407ms (17:17:38.539)
Trace[781559729]: [14m35.616523496s] [14m35.616523496s] END
I0519 17:18:07.142401       1 trace.go:236] Trace[1680046365]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:56:23.604) (total time: 1101189ms):
Trace[1680046365]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/certificates.k8s.io/v1/certificatesigningrequests?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 1072596ms (17:17:38.236)
Trace[1680046365]: [18m21.189184127s] [18m21.189184127s] END
I0519 17:18:07.231329       1 trace.go:236] Trace[598977192]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:56:14.692) (total time: 1110504ms):
Trace[598977192]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/rbac.authorization.k8s.io/v1/roles?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 1093404ms (17:17:50.132)
Trace[598977192]: [18m30.504410198s] [18m30.504410198s] END
I0519 17:18:07.232818       1 trace.go:236] Trace[1471227613]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:59:35.795) (total time: 909390ms):
Trace[1471227613]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/pods?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 880698ms (17:17:38.540)
Trace[1471227613]: [15m9.390624237s] [15m9.390624237s] END
I0519 17:18:07.029144       1 trace.go:236] Trace[613766277]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:57:52.104) (total time: 1010289ms):
Trace[613766277]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 984001ms (17:17:38.146)
Trace[613766277]: [16m50.289895028s] [16m50.289895028s] END
I0519 17:18:07.033281       1 trace.go:236] Trace[1022499741]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:56:59.204) (total time: 1065700ms):
Trace[1022499741]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/apps/v1/deployments?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 1037199ms (17:17:38.442)
Trace[1022499741]: [17m45.700701189s] [17m45.700701189s] END
I0519 17:18:07.327057       1 trace.go:236] Trace[2029045835]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 17:00:04.997) (total time: 880283ms):
Trace[2029045835]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 850600ms (17:17:37.645)
Trace[2029045835]: [14m40.283996079s] [14m40.283996079s] END
I0519 17:18:04.138396       1 trace.go:236] Trace[2020592426]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:56:51.908) (total time: 1057091ms):
Trace[2020592426]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/batch/v1/cronjobs?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 1044288ms (17:17:38.234)
Trace[2020592426]: [17m37.091931661s] [17m37.091931661s] END
I0519 17:18:07.341284       1 trace.go:236] Trace[678133874]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:58:45.305) (total time: 959994ms):
Trace[678133874]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/networking.k8s.io/v1/ingresses?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 937095ms (17:17:44.443)
Trace[678133874]: [15m59.994895937s] [15m59.994895937s] END
I0519 17:18:07.433511       1 trace.go:236] Trace[1163208720]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:51:39.127) (total time: 1385799ms):
Trace[1163208720]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 1356799ms (17:17:37.943)
Trace[1163208720]: [23m5.799807543s] [23m5.799807543s] END
I0519 17:18:07.432289       1 trace.go:236] Trace[509500435]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:51:23.525) (total time: 1398906ms):
Trace[509500435]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 1372504ms (17:17:38.044)
Trace[509500435]: [23m18.906065845s] [23m18.906065845s] END
I0519 17:18:07.437317       1 trace.go:236] Trace[342907843]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:57:22.796) (total time: 1042603ms):
Trace[342907843]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 1027398ms (17:17:52.232)
Trace[342907843]: [17m22.603153572s] [17m22.603153572s] END
W0519 17:18:07.229946       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=1532": dial tcp 192.168.49.2:8443: i/o timeout
I0519 17:18:07.027510       1 trace.go:236] Trace[169522792]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:51:51.334) (total time: 1371183ms):
Trace[169522792]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/autoscaling/v2/horizontalpodautoscalers?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 1361484ms (17:17:54.832)
Trace[169522792]: [22m51.183080902s] [22m51.183080902s] END
E0519 17:18:08.331050       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Secret: failed to list *v1.Secret: Get "https://192.168.49.2:8443/api/v1/secrets?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
W0519 17:18:08.033788       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodTemplate: Get "https://192.168.49.2:8443/api/v1/podtemplates?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
W0519 17:18:08.227367       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
W0519 17:18:08.033771       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
E0519 17:18:38.130367       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ClusterRole: failed to list *v1.ClusterRole: Get "https://192.168.49.2:8443/apis/rbac.authorization.k8s.io/v1/clusterroles?resourceVersion=1532": dial tcp 192.168.49.2:8443: i/o timeout
W0519 17:18:36.824842       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
W0519 17:18:39.835722       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ServiceAccount: Get "https://192.168.49.2:8443/api/v1/serviceaccounts?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
E0519 17:18:40.731082       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
E0519 17:18:40.528375       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
E0519 17:21:49.726894       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
E0519 17:18:59.228535       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Job: failed to list *v1.Job: Get "https://192.168.49.2:8443/apis/batch/v1/jobs?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
I0519 17:19:11.140014       1 trace.go:236] Trace[1583041990]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:59:39.782) (total time: 921700ms):
Trace[1583041990]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?resourceVersion=1532": dial tcp 192.168.49.2:8443: i/o timeout 901310ms (17:18:03.137)
Trace[1583041990]: [15m21.7007759s] [15m21.7007759s] END
I0519 17:19:09.326354       1 trace.go:236] Trace[140950982]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 17:00:19.787) (total time: 877606ms):
Trace[140950982]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/scheduling.k8s.io/v1/priorityclasses?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 861304ms (17:18:03.137)
Trace[140950982]: [14m37.606819538s] [14m37.606819538s] END
E0519 17:19:11.838474       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ValidatingAdmissionPolicy: failed to list *v1.ValidatingAdmissionPolicy: Get "https://192.168.49.2:8443/apis/admissionregistration.k8s.io/v1/validatingadmissionpolicies?resourceVersion=1532": dial tcp 192.168.49.2:8443: i/o timeout
E0519 17:19:14.724201       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/volumeattachments?resourceVersion=1532": dial tcp 192.168.49.2:8443: i/o timeout


==> kube-proxy [e1858dc15ac9] <==
W0519 16:53:16.118651       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%!D(MISSING)minikube&resourceVersion=1521": net/http: TLS handshake timeout
W0519 16:53:26.621292       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=1510": net/http: TLS handshake timeout
I0519 16:54:12.713605       1 trace.go:236] Trace[1153961415]: "iptables ChainExists" (19-May-2024 16:52:08.620) (total time: 87600ms):
Trace[1153961415]: [1m27.600764612s] [1m27.600764612s] END
I0519 16:54:29.704264       1 trace.go:236] Trace[1145180716]: "iptables ChainExists" (19-May-2024 16:52:08.515) (total time: 94200ms):
Trace[1145180716]: [1m34.200602509s] [1m34.200602509s] END
I0519 16:55:02.901587       1 trace.go:236] Trace[431142641]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:49:57.029) (total time: 246295ms):
Trace[431142641]: ---"Objects listed" error:Get "https://control-plane.minikube.internal:8443/api/v1/services?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=1510": net/http: TLS handshake timeout 196696ms (16:53:13.707)
Trace[431142641]: [4m6.295741505s] [4m6.295741505s] END
I0519 16:55:32.401165       1 trace.go:236] Trace[1042281192]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:49:57.232) (total time: 247505ms):
Trace[1042281192]: ---"Objects listed" error:Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%!D(MISSING)minikube&resourceVersion=1521": net/http: TLS handshake timeout 190199ms (16:53:07.413)
Trace[1042281192]: [4m7.505085736s] [4m7.505085736s] END
W0519 16:55:53.708750       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.EndpointSlice: Get "https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=1501": net/http: TLS handshake timeout
E0519 16:56:10.009898       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=1510": net/http: TLS handshake timeout
E0519 16:56:43.197436       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%!D(MISSING)minikube&resourceVersion=1521": net/http: TLS handshake timeout
I0519 16:56:50.999951       1 trace.go:236] Trace[1797868343]: "iptables ChainExists" (19-May-2024 16:54:39.811) (total time: 92096ms):
Trace[1797868343]: [1m32.096796578s] [1m32.096796578s] END
I0519 16:57:58.705427       1 trace.go:236] Trace[224317819]: "iptables ChainExists" (19-May-2024 16:55:08.703) (total time: 139198ms):
Trace[224317819]: [2m19.198832564s] [2m19.198832564s] END
I0519 16:57:58.702598       1 trace.go:236] Trace[1722835352]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:48:26.934) (total time: 492397ms):
Trace[1722835352]: ---"Objects listed" error:Get "https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=1501": net/http: TLS handshake timeout 436614ms (16:55:43.512)
Trace[1722835352]: [8m12.397207492s] [8m12.397207492s] END
E0519 17:00:33.999410       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=1501": net/http: TLS handshake timeout
I0519 17:01:04.186802       1 trace.go:236] Trace[1600091003]: "iptables ChainExists" (19-May-2024 16:57:09.497) (total time: 132612ms):
Trace[1600091003]: [2m12.612748897s] [2m12.612748897s] END
I0519 17:01:42.086724       1 request.go:697] Waited for 3.202934136s due to client-side throttling, not priority and fairness, request: GET:https://control-plane.minikube.internal:8443/api/v1/services?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=1510
I0519 17:02:22.976398       1 trace.go:236] Trace[756926600]: "iptables ChainExists" (19-May-2024 16:58:41.108) (total time: 175396ms):
Trace[756926600]: [2m55.396665s] [2m55.396665s] END
I0519 17:04:21.855420       1 trace.go:236] Trace[483830743]: "iptables ChainExists" (19-May-2024 17:01:38.782) (total time: 120005ms):
Trace[483830743]: [2m0.005509495s] [2m0.005509495s] END
I0519 17:05:56.145378       1 trace.go:236] Trace[690542930]: "iptables ChainExists" (19-May-2024 17:02:39.576) (total time: 150698ms):
Trace[690542930]: [2m30.698543605s] [2m30.698543605s] END
I0519 17:07:58.149123       1 trace.go:236] Trace[1075641226]: "iptables ChainExists" (19-May-2024 17:05:08.541) (total time: 125412ms):
Trace[1075641226]: [2m5.412241955s] [2m5.412241955s] END
I0519 17:08:41.842181       1 request.go:697] Waited for 8.102843647s due to client-side throttling, not priority and fairness, request: GET:https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=1501
I0519 17:09:53.228102       1 trace.go:236] Trace[340841355]: "iptables ChainExists" (19-May-2024 17:06:38.439) (total time: 123410ms):
Trace[340841355]: [2m3.410460936s] [2m3.410460936s] END
W0519 17:14:57.452909       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%!D(MISSING)minikube&resourceVersion=1521": dial tcp: lookup control-plane.minikube.internal: i/o timeout
I0519 17:15:47.946518       1 trace.go:236] Trace[1319809705]: "iptables ChainExists" (19-May-2024 17:08:39.737) (total time: 165704ms):
Trace[1319809705]: [2m45.704683625s] [2m45.704683625s] END
W0519 17:15:50.443484       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=1510": dial tcp: lookup control-plane.minikube.internal: i/o timeout
I0519 17:16:50.635434       1 trace.go:236] Trace[169472101]: "iptables ChainExists" (19-May-2024 17:10:37.523) (total time: 121891ms):
Trace[169472101]: [2m1.891448127s] [2m1.891448127s] END
I0519 17:17:33.433030       1 trace.go:236] Trace[55135279]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:57:03.414) (total time: 936174ms):
Trace[55135279]: ---"Objects listed" error:Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%!D(MISSING)minikube&resourceVersion=1521": dial tcp: lookup control-plane.minikube.internal: i/o timeout 861383ms (17:14:46.855)
Trace[55135279]: [15m36.174956212s] [15m36.174956212s] END
I0519 17:17:49.736973       1 trace.go:236] Trace[930282689]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 16:56:34.895) (total time: 1013796ms):
Trace[930282689]: ---"Objects listed" error:Get "https://control-plane.minikube.internal:8443/api/v1/services?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=1510": dial tcp: lookup control-plane.minikube.internal: i/o timeout 944116ms (17:15:41.060)
Trace[930282689]: [16m53.796985101s] [16m53.796985101s] END
E0519 17:18:16.442689       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%!D(MISSING)minikube&resourceVersion=1521": dial tcp: lookup control-plane.minikube.internal: i/o timeout
I0519 17:18:22.533226       1 trace.go:236] Trace[773430300]: "iptables ChainExists" (19-May-2024 17:16:29.740) (total time: 90307ms):
Trace[773430300]: [1m30.307478351s] [1m30.307478351s] END
I0519 17:18:22.625306       1 trace.go:236] Trace[968535087]: "iptables ChainExists" (19-May-2024 17:17:31.042) (total time: 30998ms):
Trace[968535087]: [30.998864472s] [30.998864472s] END
E0519 17:18:59.529154       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=1510": dial tcp: lookup control-plane.minikube.internal: i/o timeout
I0519 17:20:49.734658       1 trace.go:236] Trace[869769118]: "iptables ChainExists" (19-May-2024 17:18:59.932) (total time: 84003ms):
Trace[869769118]: [1m24.003101496s] [1m24.003101496s] END
I0519 17:20:49.916380       1 trace.go:236] Trace[755695002]: "iptables ChainExists" (19-May-2024 17:19:00.025) (total time: 76815ms):
Trace[755695002]: [1m16.815823814s] [1m16.815823814s] END
E0519 17:21:28.531939       1 iptables.go:589] "Could not check for iptables canary" err="exit status 4" table="mangle" chain="KUBE-PROXY-CANARY"


==> kube-scheduler [f88690a5517b] <==
E0519 17:09:43.030962       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
I0519 17:10:00.322884       1 trace.go:236] Trace[415395231]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 17:01:08.296) (total time: 476193ms):
Trace[415395231]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/nodes?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 397785ms (17:07:46.032)
Trace[415395231]: [7m56.193544546s] [7m56.193544546s] END
W0519 17:09:35.624367       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
I0519 17:10:05.033231       1 trace.go:236] Trace[261286498]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 17:02:00.877) (total time: 420896ms):
Trace[261286498]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 340913ms (17:07:41.750)
Trace[261286498]: [7m0.896820504s] [7m0.896820504s] END
W0519 17:09:58.133192       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?resourceVersion=1533": net/http: TLS handshake timeout
E0519 17:10:07.933307       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?resourceVersion=1531": dial tcp 192.168.49.2:8443: i/o timeout
E0519 17:10:10.135533       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
W0519 17:10:07.929207       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?resourceVersion=1533": net/http: TLS handshake timeout
E0519 17:10:14.624299       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?resourceVersion=1533": net/http: TLS handshake timeout
E0519 17:10:28.916211       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?resourceVersion=1533": net/http: TLS handshake timeout
E0519 17:10:40.225917       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
E0519 17:11:03.927198       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
W0519 17:10:58.114165       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
I0519 17:15:03.756178       1 trace.go:236] Trace[1254323535]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 17:01:52.580) (total time: 521713ms):
Trace[1254323535]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/namespaces?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 455701ms (17:09:28.229)
Trace[1254323535]: [8m41.713256007s] [8m41.713256007s] END
I0519 17:15:06.049028       1 trace.go:236] Trace[377199770]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 17:01:40.875) (total time: 498815ms):
Trace[377199770]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?resourceVersion=1533": net/http: TLS handshake timeout 466006ms (17:09:26.829)
Trace[377199770]: [8m18.815745383s] [8m18.815745383s] END
I0519 17:15:43.156098       1 trace.go:236] Trace[401168813]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 17:01:42.792) (total time: 592784ms):
Trace[401168813]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?resourceVersion=1533": net/http: TLS handshake timeout 500290ms (17:10:03.026)
Trace[401168813]: [9m52.784207124s] [9m52.784207124s] END
I0519 17:15:37.161968       1 trace.go:236] Trace[337797637]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 17:01:39.298) (total time: 548793ms):
Trace[337797637]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?resourceVersion=1533": net/http: TLS handshake timeout 491294ms (17:09:50.539)
Trace[337797637]: [9m8.793233042s] [9m8.793233042s] END
W0519 17:15:50.240016       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
E0519 17:15:57.745430       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?resourceVersion=1533": net/http: TLS handshake timeout
I0519 17:16:13.447676       1 trace.go:236] Trace[1568875326]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 17:02:25.674) (total time: 565696ms):
Trace[1568875326]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/services?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 507106ms (17:10:52.722)
Trace[1568875326]: [9m25.696605456s] [9m25.696605456s] END
E0519 17:16:19.450923       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?resourceVersion=1533": net/http: TLS handshake timeout
E0519 17:16:10.336514       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
E0519 17:16:57.047420       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?resourceVersion=1533": net/http: TLS handshake timeout
W0519 17:16:30.846794       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
I0519 17:17:20.035541       1 request.go:697] Waited for 2.719842035s due to client-side throttling, not priority and fairness, request: GET:https://192.168.49.2:8443/api/v1/persistentvolumeclaims?resourceVersion=1533
I0519 17:17:38.736109       1 request.go:697] Waited for 1.215243606s due to client-side throttling, not priority and fairness, request: GET:https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?resourceVersion=1531
I0519 17:17:41.433094       1 trace.go:236] Trace[1597025542]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 17:02:08.277) (total time: 651318ms):
Trace[1597025542]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 610891ms (17:15:41.239)
Trace[1597025542]: [10m51.31840361s] [10m51.31840361s] END
E0519 17:17:48.344323       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
I0519 17:17:54.935060       1 request.go:697] Waited for 1.499133005s due to client-side throttling, not priority and fairness, request: GET:https://192.168.49.2:8443/api/v1/nodes?resourceVersion=1533
I0519 17:18:16.329116       1 trace.go:236] Trace[429293031]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 17:03:22.072) (total time: 655708ms):
Trace[429293031]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 577596ms (17:16:21.738)
Trace[429293031]: [10m55.708999177s] [10m55.708999177s] END
E0519 17:18:24.427423       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
E0519 17:19:10.734818       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
W0519 17:19:53.122645       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout
W0519 17:21:43.218385       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?resourceVersion=1533": net/http: TLS handshake timeout
W0519 17:21:47.523195       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?resourceVersion=1533": net/http: TLS handshake timeout
I0519 17:21:52.516113       1 request.go:697] Waited for 4.690071357s due to client-side throttling, not priority and fairness, request: GET:https://192.168.49.2:8443/api/v1/services?resourceVersion=1533
W0519 17:21:59.413627       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?resourceVersion=1533": net/http: TLS handshake timeout
I0519 17:22:25.720590       1 trace.go:236] Trace[234371367]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (19-May-2024 17:03:58.277) (total time: 837488ms):
Trace[234371367]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=1533": dial tcp 192.168.49.2:8443: i/o timeout 737896ms (17:19:38.230)
Trace[234371367]: [13m57.488258375s] [13m57.488258375s] END
W0519 17:22:31.021732       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&resourceVersion=1533": net/http: TLS handshake timeout
W0519 17:22:06.727540       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?resourceVersion=1531": dial tcp 192.168.49.2:8443: i/o timeout


==> kubelet <==
May 21 08:04:30 minikube kubelet[51774]: I0521 08:04:30.481987   51774 server.go:484] "Kubelet version" kubeletVersion="v1.30.0"
May 21 08:04:30 minikube kubelet[51774]: I0521 08:04:30.482103   51774 server.go:486] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
May 21 08:04:30 minikube kubelet[51774]: I0521 08:04:30.482325   51774 server.go:927] "Client rotation is on, will bootstrap in background"
May 21 08:04:30 minikube kubelet[51774]: E0521 08:04:30.482364   51774 run.go:74] "command failed" err="failed to run Kubelet: unable to load bootstrap kubeconfig: stat /etc/kubernetes/bootstrap-kubelet.conf: no such file or directory"
May 21 08:04:30 minikube systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
May 21 08:04:30 minikube systemd[1]: kubelet.service: Failed with result 'exit-code'.
May 21 08:04:31 minikube systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 1563.
May 21 08:04:31 minikube systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
May 21 08:04:31 minikube systemd[1]: Started kubelet: The Kubernetes Node Agent.
May 21 08:04:31 minikube kubelet[51788]: I0521 08:04:31.229819   51788 server.go:484] "Kubelet version" kubeletVersion="v1.30.0"
May 21 08:04:31 minikube kubelet[51788]: I0521 08:04:31.229878   51788 server.go:486] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
May 21 08:04:31 minikube kubelet[51788]: I0521 08:04:31.230023   51788 server.go:927] "Client rotation is on, will bootstrap in background"
May 21 08:04:31 minikube kubelet[51788]: E0521 08:04:31.230056   51788 run.go:74] "command failed" err="failed to run Kubelet: unable to load bootstrap kubeconfig: stat /etc/kubernetes/bootstrap-kubelet.conf: no such file or directory"
May 21 08:04:31 minikube systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
May 21 08:04:31 minikube systemd[1]: kubelet.service: Failed with result 'exit-code'.
May 21 08:04:31 minikube systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 1564.
May 21 08:04:31 minikube systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
May 21 08:04:31 minikube systemd[1]: Started kubelet: The Kubernetes Node Agent.
May 21 08:04:31 minikube kubelet[51807]: I0521 08:04:31.980391   51807 server.go:484] "Kubelet version" kubeletVersion="v1.30.0"
May 21 08:04:31 minikube kubelet[51807]: I0521 08:04:31.980460   51807 server.go:486] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
May 21 08:04:31 minikube kubelet[51807]: I0521 08:04:31.982105   51807 server.go:927] "Client rotation is on, will bootstrap in background"
May 21 08:04:31 minikube kubelet[51807]: E0521 08:04:31.982298   51807 run.go:74] "command failed" err="failed to run Kubelet: unable to load bootstrap kubeconfig: stat /etc/kubernetes/bootstrap-kubelet.conf: no such file or directory"
May 21 08:04:31 minikube systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
May 21 08:04:31 minikube systemd[1]: kubelet.service: Failed with result 'exit-code'.
May 21 08:04:32 minikube systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 1565.
May 21 08:04:32 minikube systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
May 21 08:04:32 minikube systemd[1]: Started kubelet: The Kubernetes Node Agent.
May 21 08:04:32 minikube kubelet[51821]: I0521 08:04:32.727792   51821 server.go:484] "Kubelet version" kubeletVersion="v1.30.0"
May 21 08:04:32 minikube kubelet[51821]: I0521 08:04:32.727837   51821 server.go:486] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
May 21 08:04:32 minikube kubelet[51821]: I0521 08:04:32.727978   51821 server.go:927] "Client rotation is on, will bootstrap in background"
May 21 08:04:32 minikube kubelet[51821]: E0521 08:04:32.728013   51821 run.go:74] "command failed" err="failed to run Kubelet: unable to load bootstrap kubeconfig: stat /etc/kubernetes/bootstrap-kubelet.conf: no such file or directory"
May 21 08:04:32 minikube systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
May 21 08:04:32 minikube systemd[1]: kubelet.service: Failed with result 'exit-code'.
May 21 08:04:33 minikube systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 1566.
May 21 08:04:33 minikube systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
May 21 08:04:33 minikube systemd[1]: Started kubelet: The Kubernetes Node Agent.
May 21 08:04:33 minikube kubelet[51835]: I0521 08:04:33.500374   51835 server.go:484] "Kubelet version" kubeletVersion="v1.30.0"
May 21 08:04:33 minikube kubelet[51835]: I0521 08:04:33.500449   51835 server.go:486] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
May 21 08:04:33 minikube kubelet[51835]: I0521 08:04:33.500645   51835 server.go:927] "Client rotation is on, will bootstrap in background"
May 21 08:04:33 minikube kubelet[51835]: E0521 08:04:33.500691   51835 run.go:74] "command failed" err="failed to run Kubelet: unable to load bootstrap kubeconfig: stat /etc/kubernetes/bootstrap-kubelet.conf: no such file or directory"
May 21 08:04:33 minikube systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
May 21 08:04:33 minikube systemd[1]: kubelet.service: Failed with result 'exit-code'.
May 21 08:04:34 minikube systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 1567.
May 21 08:04:34 minikube systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
May 21 08:04:34 minikube systemd[1]: Started kubelet: The Kubernetes Node Agent.
May 21 08:04:34 minikube kubelet[51849]: I0521 08:04:34.240416   51849 server.go:484] "Kubelet version" kubeletVersion="v1.30.0"
May 21 08:04:34 minikube kubelet[51849]: I0521 08:04:34.240476   51849 server.go:486] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
May 21 08:04:34 minikube kubelet[51849]: I0521 08:04:34.240705   51849 server.go:927] "Client rotation is on, will bootstrap in background"
May 21 08:04:34 minikube kubelet[51849]: E0521 08:04:34.240772   51849 run.go:74] "command failed" err="failed to run Kubelet: unable to load bootstrap kubeconfig: stat /etc/kubernetes/bootstrap-kubelet.conf: no such file or directory"
May 21 08:04:34 minikube systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
May 21 08:04:34 minikube systemd[1]: kubelet.service: Failed with result 'exit-code'.
May 21 08:04:34 minikube systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 1568.
May 21 08:04:34 minikube systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
May 21 08:04:34 minikube systemd[1]: Started kubelet: The Kubernetes Node Agent.
May 21 08:04:34 minikube kubelet[51977]: I0521 08:04:34.992228   51977 server.go:484] "Kubelet version" kubeletVersion="v1.30.0"
May 21 08:04:34 minikube kubelet[51977]: I0521 08:04:34.992460   51977 server.go:486] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
May 21 08:04:34 minikube kubelet[51977]: I0521 08:04:34.993223   51977 server.go:927] "Client rotation is on, will bootstrap in background"
May 21 08:04:34 minikube kubelet[51977]: E0521 08:04:34.993293   51977 run.go:74] "command failed" err="failed to run Kubelet: unable to load bootstrap kubeconfig: stat /etc/kubernetes/bootstrap-kubelet.conf: no such file or directory"
May 21 08:04:34 minikube systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
May 21 08:04:34 minikube systemd[1]: kubelet.service: Failed with result 'exit-code'.


==> storage-provisioner [b279ab02cf1a] <==
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0xc0000951d0, 0x0, 0x18bc168, 0xc0002c4900, 0x0, 0x0, 0x461dc0, 0xc000125a40, 0xc00050fe50)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x89
k8s.io/client-go/rest/watch.(*Decoder).Decode(0xc000686780, 0xc00050fef0, 0x8, 0x18baa48, 0xc0003681c0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x6e
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0xc000650d40)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0x14a
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe

goroutine 1414 [sync.Cond.Wait]:
sync.runtime_notifyListWait(0xc000785a60, 0x5)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc000785a50)
	/usr/local/go/src/sync/cond.go:56 +0x99
golang.org/x/net/http2.(*pipe).Read(0xc000785a48, 0xc00036e001, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/pipe.go:65 +0x97
golang.org/x/net/http2.transportResponseBody.Read(0xc000785a20, 0xc00036e001, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2108 +0xaf
encoding/json.(*Decoder).refill(0xc000785e40, 0xa, 0x9)
	/usr/local/go/src/encoding/json/stream.go:165 +0xeb
encoding/json.(*Decoder).readValue(0xc000785e40, 0x0, 0x0, 0x152aee0)
	/usr/local/go/src/encoding/json/stream.go:140 +0x1ff
encoding/json.(*Decoder).Decode(0xc000785e40, 0x154a160, 0xc000182fc0, 0x203000, 0x203000)
	/usr/local/go/src/encoding/json/stream.go:63 +0x7c
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0xc000204390, 0xc0006e2c00, 0x400, 0x400, 0x40, 0x38, 0x15b0440)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x1a8
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0xc00041e8c0, 0x0, 0x18bc168, 0xc000650980, 0x0, 0x0, 0x461dc0, 0xc0004bed20, 0xc000275e50)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x89
k8s.io/client-go/rest/watch.(*Decoder).Decode(0xc000342980, 0xc000275ef0, 0x8, 0x18baa20, 0xc00044c280, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x6e
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0xc000598b80)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0x14a
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe

goroutine 1470 [sync.Cond.Wait]:
sync.runtime_notifyListWait(0xc00048b0c0, 0x4)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc00048b0b0)
	/usr/local/go/src/sync/cond.go:56 +0x99
golang.org/x/net/http2.(*pipe).Read(0xc00048b0a8, 0xc00036e601, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/pipe.go:65 +0x97
golang.org/x/net/http2.transportResponseBody.Read(0xc00048b080, 0xc00036e601, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2108 +0xaf
encoding/json.(*Decoder).refill(0xc00048b340, 0xa, 0x9)
	/usr/local/go/src/encoding/json/stream.go:165 +0xeb
encoding/json.(*Decoder).readValue(0xc00048b340, 0x0, 0x0, 0x152aee0)
	/usr/local/go/src/encoding/json/stream.go:140 +0x1ff
encoding/json.(*Decoder).Decode(0xc00048b340, 0x154a160, 0xc00049b158, 0x203000, 0x203000)
	/usr/local/go/src/encoding/json/stream.go:63 +0x7c
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0xc00052e960, 0xc0004b0000, 0x400, 0x400, 0x40, 0x38, 0x15b0440)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x1a8
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0xc00041fd10, 0x0, 0x18bc168, 0xc0002c4b00, 0x0, 0x0, 0x461dc0, 0xc000416d80, 0xc000293e50)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x89
k8s.io/client-go/rest/watch.(*Decoder).Decode(0xc000343880, 0xc000293ef0, 0x8, 0x18bbba0, 0xc00037cf00, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x6e
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0xc0005998c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0x14a
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe


==> storage-provisioner [bd82d4f75a25] <==
I0519 15:35:31.492554       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0519 15:35:52.534464       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

